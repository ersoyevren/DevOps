(https://github.com/aytitech/k8sfundamentals)

# 2. Neden Kubernetes Ã¶ÄŸrenmeniz gerekiyor?
## Neden Gerekli?


![image](https://user-images.githubusercontent.com/103413194/184480504-2fa58959-b8d6-410d-81e7-d989a4415fd9.png)

# 7. Container orchestration
Ã–rnek Ã¼zerinden gidelim:
Birden fazla microserviceâ€™in Ã§alÄ±ÅŸtÄ±ÄŸÄ± bir docker environmentâ€™Ä± dÃ¼ÅŸÃ¼nelim. Her bir microservice bir docker container iÃ§erisinde Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± ve uygulamamÄ±zÄ± kullanÄ±cÄ±lara aÃ§tÄ±ÄŸÄ±mÄ±zÄ± dÃ¼ÅŸÃ¼nelim. UygulamamÄ±z, ÅŸuan tek bir sunucu Ã¼zerinde koÅŸuyor ve sunucu Ã¼zerinde gÃ¼ncelleme yaptÄ±ÄŸÄ±mÄ±zda sistemde kesintiler oluÅŸmaya baÅŸlayacaktÄ±r.

Bu sorunumuzu Ã§Ã¶zmek adÄ±na, yeni bir sunucu kiralayÄ±p, aynÄ± docker environmentâ€™Ä±nÄ± kurgulayÄ±p (clonelayÄ±p), bir de **load balancer** kurarak daÄŸÄ±tÄ±k bir mimariye geÃ§iÅŸ yaptÄ±k. Buna raÄŸmen, docker containerâ€™lar kendiliÄŸinden kapanÄ±yor ve bu duruma manuel mÃ¼dahale etmemiz gerekir. Daha fazla trafik aldÄ±ÄŸÄ±mÄ±z iÃ§in, 2 sunucu daha kiraladÄ±k. 2 sunucu daha, 2 sunucu daha..

TÃ¼m bu sunucularÄ± **manuel yÃ¶netmeye devam ediyoruz.** Zamanla DevOps sÃ¼reÃ§lerine manuel mÃ¼dahaleden dolayÄ± Ã§ok fazla zaman harcamaya baÅŸladÄ±k, geriye kalan iÅŸlere zaman kalmadÄ±.

Peki bu durumu Ã§Ã¶zecek durum nedir? Cevap -> **Container Orchestration!**

TÃ¼m sistem konfigÃ¼rasyonlarÄ±nÄ± ayarlayÄ±p, bu karar mekanizmasÄ±nÄ± bir orkestra ÅŸefine emanet edebiliriz. Ä°ÅŸte bu ÅŸef **Kubernetesâ€™tir!**

DiÄŸer alternatifler -> Docker Swarm, Apache H2o

# 8. Kubernetes tarihÃ§esi

* Google tarafÄ±ndan geliÅŸtirilen Orchestration sistemi.
* Google, Ã§ok uzun yÄ±llardÄ±r Linux containerlar kullanÄ±yor. TÃ¼m bu containerlarÄ± yÃ¶netmek iÃ§in **Borg** adÄ±nda bir platform geliÅŸtirmiÅŸ. Fakat, zamanla hatalar Ã§Ä±kmÄ±ÅŸ ve yeni bir platform ihtiyacÄ± duyulmuÅŸ ve **Omega** platformu geliÅŸtirilmiÅŸ.
* 2013 yÄ±lÄ±nda 3 Google mÃ¼hendisi open-source bir ÅŸekilde GitHub Ã¼zerinde Kubernetes reposunu aÃ§tÄ±. _Kubernetes: Deniz dÃ¼mencisi_ _(k8s)_
* 2014 senesinde proje, Google tarafÄ±ndan CNCFâ€™e baÄŸÄ±ÅŸlandÄ±. (Cloud Native Computing Foundation)

# 9. Kubernetes nedir?

![image](https://user-images.githubusercontent.com/103413194/184481185-ad8ddc2f-9f47-4bc0-a262-0b373742d0dc.png)


* Declarative (_beyan temelli yapÄ±landÄ±rma_), Container orchestration platformu.
* Proje, hiÃ§ bir ÅŸirkete baÄŸlÄ± deÄŸil, bir vakÄ±f tarafÄ±ndan yÃ¶netilmektedir.
* Ãœcretsizdir. Rakip firmalarda **open-source**â€™dur.
* Bu kadar popÃ¼ler olmasÄ±nÄ±n sebebi, platformun tasarÄ±mÄ± ve Ã§Ã¶zÃ¼m yaklaÅŸÄ±mÄ±.
* Semantic versioning izler (_x.y.z. -> x: major, y: minor, z: patch_) ve her 4 ayda bir minor version Ã§Ä±kartÄ±r.
* Her ay patch version yayÄ±nlar.
* Bir kubernetes platformu en fazla 1 yÄ±l kullanÄ±lÄ±r, 1 yÄ±ldan sonra gÃ¼ncellemek gerekmektedir.

![](<.gitbook/assets/Screen Shot 2021-12-12 at 18.26.50.png>)

### Kubernetes TasarÄ±mÄ± ve YaklaÅŸÄ±mÄ±

Birden fazla geliÅŸtirilebilir modÃ¼llerden oluÅŸmaktadÄ±r. Bu modÃ¼llerin hepsinin bir gÃ¶revi vardÄ±r ve tÃ¼m modÃ¼ller kendi gÃ¶revlerine odaklanÄ±r. Ä°htiyaÃ§ halinde bu modÃ¼ller veya yeni modÃ¼ller geliÅŸtirilebilir. (extendable)

K8s, â€œ_Åunu yap, daha sonra ÅŸunu yap_â€ gibi adÄ±m adÄ±m ne yapmamÄ±z gerektiÄŸini sÃ¶ylemek yerine _**(imperative yÃ¶ntem);** â€œBen ÅŸÃ¶yle bir ÅŸey istiyorumâ€ **(declarative yÃ¶ntem)**_ yaklaÅŸÄ±mÄ± sunmaktadÄ±r. **NasÄ±l yapÄ±lacaÄŸÄ±nÄ± tarif etmiyor, ne istediÄŸimizi sÃ¶ylÃ¼yoruz.**

* Imperative yÃ¶ntem, bize zaman kaybettirir, tÃ¼m adÄ±mlarÄ± tasarlamak zorunda kalÄ±rÄ±z.
* Declarative yÃ¶ntemâ€™de ise sadece ne istediÄŸimizi sÃ¶ylÃ¼yoruz ve sonuca bakÄ±yoruz.

Kubernetes bize ondan istediÄŸimizi soruyor, biz sÃ¶ylÃ¼yoruz ve bizim istediklerimizin dÄ±ÅŸÄ±na Ã§Ä±kmÄ±yor. Ã–rneÄŸin, **Desired State (Deklara Edilen Durum - \_Ä°stekler gibi dÃ¼ÅŸÃ¼nebiliriz**\_**)** ÅŸu ÅŸekilde olsun:

* Berksafran/k8s:latest isimli imaj yaratÄ±p, 10 containerla Ã§alÄ±ÅŸtÄ±r. DÄ±ÅŸ dÃ¼nyaya 80 portunu aÃ§ ve ben bu serviceâ€™e bir gÃ¼ncelleme geÃ§tiÄŸimde aynÄ± anda 2 task Ã¼zerinde yÃ¼rÃ¼t ve 10sn beklensin.

Kubernetes, 9 container Ã§alÄ±ÅŸÄ±r durumda kalÄ±rsa hemen bir container daha ayaÄŸa kaldÄ±rÄ±r ve isteklerimize gÃ¶re platformu **optimize eder.** Bu bizi, Ã§ok bÃ¼yÃ¼k bir iÅŸten kurtarÄ±yor. _(Dockerâ€™da manuel olarak ayaÄŸa kaldÄ±rÄ±yorduk, hatÄ±rlayÄ±n.)_

# 10. Kubernetes komponentleri - 1

K8s, **microservice mimarisi dikkate alÄ±narak** oluÅŸturulmuÅŸtur.

![image](https://user-images.githubusercontent.com/103413194/184483333-38e7d509-1fa2-411c-98f8-c54ad561add0.png)


![](<.gitbook/assets/Screen Shot 2021-12-12 at 19.23.15.png>)

### **Control Plane** (Master Nodes)

AÅŸaÄŸÄ±daki, 4 component k8s yÃ¶netim kÄ±smÄ±nÄ± oluÅŸturur ve **master-node** Ã¼zerinde Ã§alÄ±ÅŸÄ±r.

* **Master-node** -> YÃ¶netim modullerinin Ã§alÄ±ÅŸtÄ±ÄŸÄ± yerdir.
* **Worker-node** -> Ä°ÅŸ yÃ¼kÃ¼nÃ¼n Ã§alÄ±ÅŸtÄ±ÄŸÄ± yerdir.
* 
![](<.gitbook/assets/Screen Shot 2022-07-19 at 12.17.04.png>)
![](<.gitbook/assets/Screen Shot 2021-12-12 at 18.48.28.png>)

* **kube-apiserver** **(api) â€“>** K8sâ€™in beyni, **ana haberleÅŸme merkezi, giriÅŸ noktasÄ±dÄ±r**. Bir nev-i **Gateway** diyebiliriz. TÃ¼m **componentler** ve **node**â€™lar, **kube-apiserver** Ã¼zerinden iletiÅŸim kurar. AyrÄ±ca, dÄ±ÅŸ dÃ¼nya ile platform arasÄ±ndaki iletiÅŸimi de **kube-apiserver** saÄŸlar. Bu denli herkesle iletiÅŸim kurabilen **tek componenttir**. **Authentication ve Authorization** gÃ¶revini Ã¼stlenir.

* **etcd** **->** K8sâ€™in tÃ¼m cluster verisi, metada bilgileri ve Kubernetes platformunda oluÅŸturulan componentlerin ve objelerin bilgileri burada tutulur. Bir nevi **ArÅŸiv odasÄ±.** etcd, **key-value** ÅŸeklinde datalarÄ± tutar. DiÄŸer componentler, etdc ile **direkt haberleÅŸemezler.** Bu iletiÅŸimi, **kube-apiserver** aracÄ±lÄ±ÄŸÄ±yla yaparlar.
* 
* **kube-scheduler (sched) ->** K8sâ€™i Ã§alÄ±ÅŸma planlamasÄ±nÄ±n yapÄ±ldÄ±ÄŸÄ± yer. Yeni oluÅŸturulan ya da bir node atamasÄ± yapÄ±lmamÄ±ÅŸ Podâ€™larÄ± izler ve Ã¼zerinde Ã§alÄ±ÅŸacaklarÄ± bir **node** seÃ§er. (_Pod = container_) Bu seÃ§imi yaparken, CPU, Ram vb. Ã§eÅŸitli parametreleri deÄŸerlendirir ve **bir seÃ§me algoritmasÄ± sayesinde pod iÃ§in en uygun nodeâ€™un hangisi olduÄŸuna karar verir.**
* 
* **kube-controller-manager (c-m) ->** K8sâ€™in kontrollerinin yapÄ±ldÄ±ÄŸÄ± yapÄ±dÄ±r. **Mevcut durum ile istenilen durum arasÄ±nda fark olup olmadÄ±ÄŸÄ±nÄ± denetler.** Ã–rneÄŸin; siz 3 cluster istediniz ve k8s bunu gerÃ§ekleÅŸtirdi. Fakat bir sorun oldu ve 2 container kaldÄ±. kube-controller, burada devreye girer ve hemen bir cluster daha ayaÄŸa kaldÄ±rÄ±r. Tek bir binary olarak derlense de iÃ§erisinde bir Ã§ok controller barÄ±ndÄ±rÄ±r:
  * Node Controller,
  * Job Controller,
  * Service Account & Token Controller,
  * Endpoints Controller.

### **Worker Nodes**

ContainerlarÄ±mÄ±zÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ± yerlerdir. Container veya Docker gibi containerâ€™lar Ã§alÄ±ÅŸtÄ±rÄ±r. Her worker nodeâ€™da **3 temel component** bulunur:

1. **Container runtime ->** VarsayÄ±lan olarak Dockerâ€™dÄ±r. Ama Ã§eÅŸitli sebeplerden dolayÄ± **Dockerâ€™dan Containerdâ€˜ye** geÃ§miÅŸtir. Docker ve containerd arasÄ±nda fark yok nedebilecek kadar azdÄ±r. Hatta Docker kendi iÃ§erisinde de containerd kullanÄ±r. DiÄŸer desteklenen container Ã§eÅŸidi **CRI-Oâ€™dur.**

3. **kubelet ->** API Server aracÄ±lÄ±ÄŸÄ±yla etcdâ€™yi kontrol eder ve sched tarafÄ±ndan bulunduÄŸu node Ã¼zerinde Ã§alÄ±ÅŸmasÄ± gereken podlarÄ± yaratÄ±r. Containerdâ€™ye haber gÃ¶nderir ve belirlenen Ã¶zelliklerde bir container Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.

5. **kube-proxy ->** Nodelar Ã¼stÃ¼nde aÄŸ kurallarÄ±nÄ± ve trafik akÄ±ÅŸÄ±nÄ± yÃ¶netir. Podâ€™larla olan iletiÅŸime izin verir, izler.

TÃ¼m bunlarÄ±n dÄ±ÅŸÄ±nda GUI hizmeti saÄŸlayan vb. pluginler de kurulmaktadÄ±r.

# 13. Kubectl kurulumu

TÃ¼m komutlar, kube-apiserver Ã¼zerinden verilir. Bu komutlar 3 ÅŸekilde verilebilir:

1. **REST Api** aracÄ±lÄ±ÄŸÄ±yla (terminalden curl olarak vb.),
2. **Kubernetes GUI** (Dashboard, Lens, Octant) Ã¼zerinden,
3. **Kubectl** aracÄ±lÄ±ÄŸÄ±yla (CLI).

## Kubectl Kurulumu

* Homebrew ile aÅŸaÄŸÄ±daki komutu yazÄ±yoruz. (kubernetes.ioâ€™da farklÄ± kurulumlar mevcut)

```
brew install kubectl
```

* Test iÃ§in `kubectl version` yazabilirsiniz. (Server baÄŸlantÄ±sÄ± yapÄ±lmadÄ±ÄŸÄ± iÃ§in o kÄ±sÄ±mda hata alÄ±nabilir, normaldir.)

## Kubernetes Kurulumu

### Hangi versionâ€™u kuracaÄŸÄ±z?

* En light-weight version iÃ§in -> **minikube**, **Docker Desktop (Single Node K8s Cluster)**
* DiÄŸer seÃ§enekler -> **kubeadm, kubespray**
* Cloud Ã§Ã¶zÃ¼mler -> **Azure Kubernetes Service (AKS), Google Kubernetes Engine, Amazon EKS**

### Docker Desktop

* Docker Desktop, Single Node Kubernetes Cluster ayaÄŸa kaldÄ±rmaya imkan tanÄ±yor. Bu durum, **baÅŸka bir araca duymadan Kubernetes Ã¼zerinde iÅŸlem yapabilme yeteneÄŸi kazandÄ±rÄ±yor.** Ama tavsiye olarak **minikube kullanÄ±lmasÄ±dÄ±r!**
* Docker Desktop iÃ§erisinde K8s kurulumu iÃ§in, Settings > Kubernetesâ€™e gidip install etmeniz gerekiyor.

### :large\_blue\_diamond: Minikube

* Bir Ã§ok addon ile gelebiliyor. Tek bir komut ile clusterâ€™Ä± durdurup, Ã§alÄ±ÅŸtÄ±rabiliyoruz.

```
brew install minikube
```

* **minikube kullanabilmek iÃ§in sistemde Docker yÃ¼klÃ¼ olmasÄ± gerekiyor.** Ã‡Ã¼nkÃ¼, Minikube backgroundâ€™da Dockerâ€™Ä± kullanacaktÄ±r. VirtualBox gibi bir Ã§ok toolâ€™u da background olarak kullanabiliriz.
* Test iÃ§in `minikube status`

#### **Minikube Ã¼zerinde K8s Cluster Kurulumu**

VarsayÄ±lan olarak Dockerâ€™Ä± backgroundâ€™da kullanÄ±r.

```shell
minikube start

minikube start --driver=virtualbox # VirtualBox background'Ä±nda Ã§alÄ±ÅŸtÄ±rmak iÃ§in.
```

Test iÃ§in

```shell
minikube status
kubectl get nodes
```

**Kubernetes clusterâ€™Ä± ve iÃ§eriÄŸini (tÃ¼m podlarÄ±) silmek iÃ§in**

```shell
minikube delete
```

**Kubernetes clusterâ€™Ä±nÄ± durdurmak iÃ§in**

```shell
minikube stop
```

## :warning::warning:\[WIP] kubeadm Kurulumu

\-> Kubernetes clusterâ€™Ä± oluÅŸturmamÄ±zÄ± saÄŸlayan baÅŸka bir platformdur. minikubeâ€™e gÃ¶re daha geliÅŸmiÅŸtir. Rassbery Pi Ã¼zerinde de Ã§alÄ±ÅŸabilir :)

> **Buraya yazÄ±lacak diÄŸer tutorialâ€™lar:**
>
> * Google Cloud Platformâ€™unda Kurulum,
> * AWS'de Kurulum,
> * Azure'da Kurulum.

## Play-with-kubernetes Kurulumu

* EÄŸer cloud iÃ§in kredi kartÄ±nÄ±zÄ± vermek istemiyorsanÄ±z ya da hÄ±zlÄ±ca bazÄ± denemeler yapmak istiyorsanÄ±z, **play-with-kubernetes** tam size gÃ¶re.
* 4 saatlik kullanÄ±m sÄ±nÄ±rÄ± var. 4 saat sonra sistem sÄ±fÄ±rlanÄ±yor, ayarlar gidiyor.
* Browser based Ã§alÄ±ÅŸÄ±r.
* Toplam max 5 tane node oluÅŸturabiliyorsunuz.

## Tools

* **Lens -->** Kubernetes iÃ§in Ã§ok iyi hazÄ±rlanmÄ±ÅŸ bir yÃ¶netim tool'u.
* **kubectx -->** HÄ±zlÄ± config/context geÃ§iÅŸi iÃ§in.
* **Krew -->** kubectl iÃ§in plugin-set'leri

# 22. Kubectl config

## Kubectl

![](<.gitbook/assets/Screen Shot 2021-12-12 at 23.53.55.png>)

![image](https://user-images.githubusercontent.com/103413194/184497612-6c6f651c-84b9-4b2b-87bc-f5b5653709a3.png)


* kubectl ile mevcut clusterâ€™Ä± yÃ¶netimini **config** dosyasÄ± Ã¼zerinden yapmamÄ±z gerekir. Minikube gibi toolâ€™lar config dosyalarÄ±nÄ± otomatik olarak oluÅŸturur.
* **Default config** dosyasÄ±na `nano ~/.kube/config` yazarak ulaÅŸabiliriz.
* VSCodeâ€™da aÃ§mak iÃ§in

```
 cd ~/.kube 
 code .
```

* **context ->** Cluster ile user bilgilerini birleÅŸtirerek context bilgisini oluÅŸturur. â€œ_Bu clusterâ€™a bu userâ€™la baÄŸlanacaÄŸÄ±m._â€ anlamÄ±na geliyor.

![image](https://user-images.githubusercontent.com/103413194/184497762-3d8df7c7-13e4-45be-b315-ac1378f9e184.png)


### Kubectl Config KomutlarÄ±

**`kubectl config`**

â€“> Config ayarlarÄ±nÄ±n dÃ¼zenlenmesini saÄŸlayan komuttur.

**`kubectl config get-contexts`**

â€“> Kubectl baktÄ±ÄŸÄ± config dosyasÄ±ndaki mevcut contextleri listeler. Mevcut olan contexin basinda yildiz var.

#### **Current Context**

Current sÃ¼tununda minikubeâ€™un yanÄ±nda bir yÄ±ldÄ±z (\*) iÅŸareti gÃ¶rÃ¼nÃ¼r. Bunun anlamÄ±: birden fazla context tanÄ±mlansa da **o an kullanÄ±lan context** anlamÄ±na gelir. YapacaÄŸÄ±mÄ±z tÃ¼m iÅŸlemleri bu contextâ€™in iÃ§erisinde gerÃ§ekleÅŸecektir.

**`kubectl config current-context`**

â€“> Kubectl baktÄ±ÄŸÄ± config dosyasÄ±ndaki current contextâ€™i verir.

**`kubectl config use-context <contextName>`**

â€“> Current contextâ€™i contextName olarak belirtilen contextâ€™i ile deÄŸiÅŸtirir.

Ã–R: `kubectl config use-context docker-desktop` â€“> docker-desktop contextâ€™ine geÃ§er.

# 23. Kubectl kullanÄ±mÄ±

**`kubectl`** 

bize kubectl ile kullanabilecegimiz komutlari veriyor.

## Kubectl KullanÄ±m KomutlarÄ±

* kubectlâ€™de komutlar belli bir ÅŸemayla tasarlanmÄ±ÅŸtÄ±r:

```
 kubectl <fiil> <object> â€‹
 
 # <fiil> = get, delete, edit, apply 
 # <object> = pod
```

* kubectlâ€™de aksi belirtilmedikÃ§e tÃ¼m komutlar **configâ€™de yazÄ±lan namespaceâ€™ler** Ã¼zerinde uygulanÄ±r. Configâ€™de namespace belirtilmediyse, **default namespace** geÃ§erlidir.

### `kubectl cluster-info`

â€“> Ãœzerinde iÅŸlem yaptÄ±ÄŸÄ±mÄ±z **cluster** ile ilgili bilgileri Ã¶ÄŸreneceÄŸimiz komut.

### `kubectl get pods`

â€“> **Default namespace**â€˜deki podâ€™larÄ± getirir.

### `kubectl get pods testpod`

â€“> **testpod** isimli podâ€™u getirir.

### **`kubectl get pods -n kube-system`**

â€“> **kube-system** isimli namespaceâ€™de tanÄ±mlanmÄ±ÅŸ podâ€™larÄ± getirir.

### `kubectl get pods -A`

â€“> **TÃ¼m namespacelerdeki** podâ€™larÄ± getirir.

### `kubectl get pods -A -o <wide|yaml|json>`

â€“> **TÃ¼m namespacelerdeki** podâ€™larÄ± **istenilen output formatÄ±nda** getirir.

```shell
# jq -> json query pluginin kur.
# brew install jq

kubectl get pods -A -o json | jq -r ".items[].spec.containers[].name"
```

### `kubectl apply --help`

â€“> **apply** komutunun nasÄ±l kullanÄ±lacaÄŸÄ± ile ilgili bilgi verir. Ama `kubectl pod â€“-help` yazsak, bu pod ile ilgili **bilgi vermez.** Bunun yerine aÅŸaÄŸÄ±daki **explain** komutu kullanÄ±lmalÄ±dÄ±r.

### `kubectl explain pod`

\--> **pod** objesinin ne olduÄŸunu, hangi fieldâ€™larÄ± aldÄ±ÄŸÄ±nÄ± gÃ¶sterir.

\--> Ã‡Ä±kan output'ta **Version** ile hangi namespaceâ€™e ait olduÄŸunu anlayabiliriz.

### `kubectl get pods`<mark style="color:red;">`-w`</mark>

\--> kubectl'i izleme (watch) moduna alÄ±r ve deÄŸiÅŸimlerin canlÄ± olarak izlenmesini saÄŸlar.

### `kubectl get all`<mark style="color:red;">`-A`</mark>

\--> Sistemde Ã§alÄ±ÅŸan **tÃ¼m object'lerin durumunu** gÃ¶sterir.

### `kubectl exec -it <podName> -c <containerName> -- bash`

\--> Pod iÃ§erisinde Ã§alÄ±ÅŸan bir container'a bash ile baÄŸlanmak iÃ§in.

## HÄ±zlÄ± Kubectl Config DeÄŸiÅŸtirme

HÄ±zlÄ±ca config deÄŸiÅŸtirmek iÃ§in aÅŸaÄŸÄ±daki bash scriptten yararlanabiliriz:

{% code title="change.sh" %}
```bash
#! /bin/bash

CLUSTER=$1

if [ -z "$1" ]
  then
    echo -e "\n##### No argument supplied. Please select one of these configs. #####"
    ls  ~/.kube |grep config- | cut -d "-" -f 2
    echo -e "######################################################################\n"
    #array=($(ls -d * |grep config_))
    read -p 'Please set config file: ' config
    cp -r ~/.kube/config_$config ~/.kube/config
    echo -e '\n'
    kubectl cluster-info |grep -v "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'."
    kubectl config get-contexts
    kubectl get node -o wide |head -n 4
else
  cp -r ~/.kube/config-$CLUSTER ~/.kube/config
  if [ $? -ne 0 ];
  then
  exit 1
  fi
  echo -e '\n'
#  kubectl cluster-info |grep -v "To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'."
  kubectl config get-contexts
  echo -e '\n'
  kubectl get node -o wide |head -n 4
  echo -e '\n'
fi
```
{% endcode %}

KullanÄ±m:

\--> Config dosyasÄ± `config-minikube` ÅŸeklinde oluÅŸturulmalÄ±dÄ±r. Script Ã§alÄ±ÅŸtÄ±rÄ±rken config prefix'i ekliyor.

```
./change.sh <configName>

./change.sh minikube
```


# 24. Pod


## Pod Nedir?

* K8s Ã¼zerinde koÅŸturduÄŸumuz, Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±z, deploy ettiÄŸimiz ÅŸeylere **Kubernetes Object** denir.
* **En temel object Podâ€˜dur.**
* K8sâ€™den Ã¶nce hep docker container ile Ã§alÄ±ÅŸtÄ±k. **K8sâ€™de ise biz, direkt olarak container oluÅŸturmayÄ±z.** K8s DÃ¼nyaâ€™sÄ±nda oluÅŸturabileceÄŸimiz, yÃ¶netebileceÄŸimiz **en kÃ¼Ã§Ã¼k birim Podâ€™dur.**
* Podâ€™lar **bir veya birden** fazla container barÄ±ndÄ±rabilir. **Best Practice iÃ§in her bir pod bir tek container barÄ±ndÄ±rÄ±r.**
* Her podâ€™un **unique bir IDâ€™si (uid)** vardÄ±r ve **unique bir IPâ€™si** vardÄ±r. Api-server, bu uid ve IPâ€™yi **etcdâ€™ye kaydeder.** Scheduler ise herhangi bir podun node ile iliÅŸkisi kurulmadÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼rse, o podu Ã§alÄ±ÅŸtÄ±rmasÄ± iÃ§in **uygun bir worker node** seÃ§er ve bu bilgiyi pod tanÄ±mÄ±na ekler. Pod iÃ§erisinde Ã§alÄ±ÅŸan **kubelet** servisi bu pod tanÄ±mÄ±nÄ± gÃ¶rÃ¼r ve ilgili containerâ€™Ä± Ã§alÄ±ÅŸtÄ±rÄ±r.
* AynÄ± pod iÃ§erisindeki containerlar aynÄ± node Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r ve bu containerlar localhost Ã¼zerinden haberleÅŸir.
* **`kubectl run`** ÅŸeklinde pod oluÅŸturulur.

# 25. Pod oluÅŸturma

```shell
kubectl run firstpod --image=nginx --restart=Never --port=80 --labels="app=frontend" 

# Output: pod/firstpod created
```

* `â€“restart` -> EÄŸer pod iÃ§erisindeki container bir sebepten Ã¶tÃ¼rÃ¼ durursa, tekrar Ã§alÄ±ÅŸtÄ±rÄ±lmamasÄ± iÃ§in `Never` yazdÄ±k.

### TanÄ±mlanan PodlarÄ± GÃ¶sterme

```shell
kubectl get pods -o wide

# -o wide --> Daha geniÅŸ table gÃ¶sterimi iÃ§in.
```

### Bir Objectâ€™in DetaylarÄ±nÄ± GÃ¶rmek

```shell
kubectl describe <object> <objectName>

kubectl describe pods first-pod
```

* `first-pod` podu ile ilgili tÃ¼m bilgileri getirir.
* Bilgiler iÃ§erisinde **Events**â€˜e dikkat. Podâ€™un tarihÃ§esi, neler olmuÅŸ, k8s neler yapmÄ±ÅŸ gÃ¶rebiliriz.
  * Ã–nce Scheduler node atamasÄ± yapar,
  * kubelet container imageâ€™Ä± pullâ€™lamÄ±ÅŸ,
  * kubelet pod oluÅŸturulmuÅŸ.

### Pod LoglarÄ±nÄ± GÃ¶rmek

```shell
kubectl logs <podName>

kubectl logs first-pod
```

**LoglarÄ± CanlÄ± Olarak (Realtime) GÃ¶rmek**

```shell
kubectl logs -f <podName>
```

### Pod Ä°Ã§erisinde Komut Ã‡alÄ±ÅŸtÄ±rma

```
kubectl exec <podName> -- <command>

kubectl exec first-pod -- ls /
```

### Pod Ä°Ã§erisindeki Containerâ€™a BaÄŸlanma

```shell
kubectl exec -it <podName> -- <shellName>

kubectl exec -it first-pod -- /bin/sh
```

**EÄŸer 1 Pod iÃ§erisinde 1'den fazla container varsa:**

```
kubectl exec -it <podName> -c <containerName> -- <bash|/bin/sh>
```

\-> BaÄŸlandÄ±ktan sonra kullanabileceÄŸimiz bazÄ± komutlar:

```shell
hostname #pod ismini verir.
printenv #Pod env variables'larÄ± getirir.
```

\--> EÄŸer bir pod iÃ§erisinde birden fazla container varsa, istediÄŸimiz container'a baÄŸlanmak iÃ§in:

```
kubectl exec -it <podName> -c <containerName> -- /bin/sh
```

### Podâ€™u Silme

```shell
kubectl delete pods <podName>
```

\-> Silme iÅŸlemi yaparken dikkat! Ã‡Ã¼nkÃ¼, confirm almÄ±yor. **Direkt siliyor!** Ã–zellikle productionâ€™da dikkat!

## YAML

* k8s, declarative yÃ¶ntem olarak **YAML** veya **JSON** destekler.
* **`---` (Ã¼Ã§ tire) koyarak bir YAML dosyasÄ± iÃ§erisinde birden fazla object belirletebiliriz.**

_k8sfundamentals/pod/objectbasetemplate.yaml dosyasÄ±nÄ± aÃ§tÄ±k._

```yaml
apiVersion:
kind:
metadata:
spec:
```

* Her tÃ¼rlÃ¼ object oluÅŸturulurken; **apiVersion, kind ve metadata** olmak zorundadÄ±r.
* **`kind`** â€“> Hangi object tÃ¼rÃ¼nÃ¼ oluÅŸturmak istiyorsak buraya yazarÄ±z. Ã–R: `pod`
* **`apiVersion`** â€“> OluÅŸturmak istediÄŸimiz objectâ€™in hangi API Ã¼zerinde ya da endpoint Ã¼zerinde sunulduÄŸunu gÃ¶sterir.
* **`metadata`** â€“> Object ile ilgili unique bilgileri tanÄ±mladÄ±ÄŸÄ±mÄ±z yerdir. Ã–R: `namespace`, `annotation` vb.
* **`spec`** â€“> OluÅŸturmak istediÄŸimiz objectâ€™in Ã¶zelliklerini belirttiÄŸimiz yerdir. Her object iÃ§in gireceÄŸimiz bilgiler farklÄ±dÄ±r. Burada yazacaÄŸÄ±mÄ±z tanÄ±mlarÄ±, dokÃ¼mantasyondan bakabiliriz.

### apiVersionâ€™u Nereden BulacaÄŸÄ±z?

1. DokÃ¼mantasyona bakabiliriz.
2. kubectl aracÄ±lÄ±ÄŸÄ±yla Ã¶ÄŸrenebiliriz:

```shell
kubectl explain pods
```

YukarÄ±daki explain komutunu yazarak podâ€™un Ã¶zelliklerini Ã¶ÄŸrenebiliriz.

â€“> `Versions` karÅŸÄ±sÄ±nda yazan bizim `apiVersion`umuzdur.

### metadata ve spec YazÄ±mÄ±

> _AÅŸaÄŸÄ±daki yamlâ€™Ä± k8sfundamentals/pod/pod1.yaml dosyasÄ±nda gÃ¶rebilirsin._

```yaml
apiVersion: v1
kind: Pod
metadata:
	name: first-pod 	# Pod'un ismi
	labels:			# AtayacaÄŸÄ±mÄ±z etiketleri yazabiliriz.
		app: front-end  # app = front-end label'i oluÅŸturduk.
spec:	
	containers: 			# Container tanÄ±mlamalarÄ± yapÄ±yoruz.
	- name: nginx			# Container ismi
		image: nginx:latest
		ports:
		- containerPort: 80 # Container'e dÄ±ÅŸarÄ±dan eriÅŸilecek port
```

### K8sâ€™e YAML DosyasÄ±nÄ± Declare Etme

```shell
kubectl apply -f pod1.yaml
```

* pod1.yaml dosyasÄ±nÄ± al ve burada tanÄ±mlanan objectâ€™i benim iÃ§in oluÅŸtur.
* `kubectl describe pods firstpod` ile oluÅŸturulan podâ€™un tÃ¼m Ã¶zellikleri gÃ¶rÃ¼ntÃ¼lenir.
* YAML yÃ¶ntemi kullanmak pipelineâ€™da yaratmakta kullanÄ±labilir.
* :thumbsup: **Declarative YÃ¶ntem AvantajÄ±** â€“> Imperative yÃ¶ntemle bir pod tanÄ±mladÄ±ÄŸÄ±mÄ±zda, bir Ã¶zelliÄŸini update etmek istediÄŸimizde â€œAlready exists.â€ hatasÄ± alÄ±rÄ±z. Ama declarative olarak YAMLâ€™Ä± deÄŸiÅŸtirip, update etmek isteseydik ve apply komutunu kullansaydÄ±k â€œpod configuredâ€ success mesajÄ±nÄ± alÄ±rÄ±z.

### Declare Edilen YAML DosyasÄ±nÄ± Durdurma

```shell
kubectl delete -f pod1.yaml
```

### Kubectl ile Podâ€™larÄ± Direkt DeÄŸiÅŸtirmek (Edit)

```shell
kubectl edit pods <podName>
```

* Herhangi tanÄ±mlanmÄ±ÅŸ bir podâ€™un Ã¶zelliklerini direkt deÄŸiÅŸtirmek iÃ§in kullanÄ±lÄ±r.
* `i`tuÅŸuna basarak `INSERT`moduna geÃ§ip, editleme yaparÄ±z.
* CMD + C ile Ã§Ä±kÄ±p, `:wq` ile VIMâ€™den Ã§Ä±kabiliriz.
* Podâ€™un editlendiÄŸi mesajÄ±nÄ± gÃ¶rÃ¼rÃ¼z.
* Tercih edilen bir yÃ¶ntem deÄŸildir, YAML + `kubectl apply` tercih edilmelidir.

## 28. Pod YaÅŸam DÃ¶ngÃ¼sÃ¼

* **Pending** â€“> Pod oluÅŸturmak iÃ§in bir YAML dosyasÄ± yazdÄ±ÄŸÄ±mÄ±zda, YAML dosyasÄ±nda yazan configlerle varsayÄ±lanlar harmanlanÄ±r ve etcdâ€™ye kaydolur.
* **Creating** â€“> kube-sched, etcdâ€™yi sÃ¼rekli izler ve herhangi bir nodeâ€™a atanmamÄ±ÅŸ pod gÃ¶rÃ¼lÃ¼rse devreye girer ve en uygun nodeâ€™u seÃ§er ve node bilgisini ekler. EÄŸer bu aÅŸamada takÄ±lÄ± kalÄ±yorsa, **uygun bir node bulunamadÄ±ÄŸÄ± anlamÄ±na gelir.**
  * etcdâ€™yi sÃ¼rekli izler ve bulunduÄŸu nodeâ€™a atanmÄ±ÅŸ podlarÄ± tespit eder. Buna gÃ¶re containerlarÄ± oluÅŸturmak iÃ§in imageâ€™leri download eder. EÄŸer image bulunamazsa veya repodan Ã§ekilemezse **ImagePullBackOff** durumuna geÃ§er.
  * EÄŸer image doÄŸru bir ÅŸekilde Ã§ekilir ve containerlar oluÅŸmaya baÅŸlarsa Pod **Running** durumuna geÃ§er.

> _Burada bir S verelim.. Containerâ€™larÄ±n Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±ndan bahsedelim:_

* Container imagelarÄ±nda sÃ¼rekli Ã§alÄ±ÅŸmasÄ± gereken bir uygulama bulunur. Bu uygulama Ã§alÄ±ÅŸtÄ±ÄŸÄ± sÃ¼rece container da Ã§alÄ±ÅŸÄ±r durumdadÄ±r. Uygulama 3 ÅŸekilde Ã§alÄ±ÅŸmasÄ±nÄ± sonlandÄ±rÄ±r:
  1. Uygulama tÃ¼m gÃ¶revlerini tamamlar ve hatasÄ±z kapanÄ±r.
  2. KullanÄ±cÄ± veya sistem kapanma sinyali gÃ¶nderir ve hatasÄ±z kapanÄ±r.
  3. Hata verir, Ã§Ã¶ker, kapanÄ±r.

> _DÃ¶ngÃ¼ye geri dÃ¶nelim.._

* Container uygulamasÄ±nÄ±n durmasÄ±na karÅŸÄ±lÄ±k, Pod iÃ§erisinde bir **RestartPolicy** tanÄ±mlanÄ±r ve 3 deÄŸer alÄ±r:
  * **`Always`** -> Kubelet bu containerâ€™Ä± yeniden baÅŸlatÄ±r.
  * **`Never`** -> Kubelet bu containerâ€™Ä± yeniden **baÅŸlatmaz**.
  * **`On-failure`** -> Kubelet sadece container hata alÄ±nca baÅŸlatÄ±r.\
* **Succeeded** -> Pod baÅŸarÄ±yla oluÅŸturulmuÅŸsa bu duruma geÃ§er.
* **Failed** -> Pod baÅŸarÄ±yla oluÅŸturulmamÄ±ÅŸsa bu duruma geÃ§er.
* **Completed** -> Pod baÅŸarÄ±yla oluÅŸturulup, Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r ve hatasÄ±z kapanÄ±rsa bu duruma geÃ§er.
* :warning: **CrashLookBackOff** -> Pod oluÅŸturulup sÄ±k sÄ±k kapanÄ±yorsa ve RestartPolicyâ€™den dolayÄ± sÃ¼rekli yeniden baÅŸlatÄ±lmaya Ã§alÄ±ÅŸÄ±lÄ±yorsa, k8s bunu algÄ±lar ve bu podu bu stateâ€™e getirir. Bu state de olan **podlar incelenmelidir.**

# 29. Pod yaÅŸam dÃ¶ngÃ¼sÃ¼ uygulama

**kubectl get pods -w**
canli olarak degisimi izliyoruz.

![image](https://user-images.githubusercontent.com/103413194/184527496-98829e3e-888f-40b3-a775-a7746c89c413.png)


-c komutu ile 

# 30. Ã‡oklu container pod

### **Neden 2 uygulamayÄ± aynÄ± containerâ€™a koymuyoruz?**

â€“> Cevap: Ä°zolasyon. 2 uygulama izole Ã§alÄ±ÅŸsÄ±n. EÄŸer bu izolasyonu saÄŸlamazsanÄ±z, yatay scaling yapamazsÄ±nÄ±z. Bu durumu Ã§oklamak gerektiÄŸinde ve 2. containeri aldÄ±ÄŸÄ±mÄ±zda 2 tane MySQL, 2 tane Wordpress olacak ki bu iyi bir ÅŸey deÄŸil.

:ok\_hand: Bu sebeple **1 Pod = 1 Container = 1 uygulama** olmalÄ±dÄ±r! DiÄŸer senaryolar **Anti-Pattern** olur.

### Peki, neden podâ€™lar neden multi-containerâ€™a izin veriyor?

â€“> Cevap: BazÄ± uygulamalar bÃ¼tÃ¼nleÅŸik (baÄŸÄ±mlÄ±) Ã§alÄ±ÅŸÄ±r. Yani ana uygulama Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda Ã§alÄ±ÅŸmalÄ±, durduÄŸunda durmalÄ±dÄ±r. Bu tÃ¼r durumlarda bir pod iÃ§erisine birden fazla container koyabiliriz.

â€“> Bir pod iÃ§erisindeki 2 container iÃ§in network gerekmez, localhost Ã¼zerinden Ã§alÄ±ÅŸabilir.

\-> EÄŸer multi-containerâ€™a sahip pod varsa ve bu containerlardan birine baÄŸlanmak istersek:

```shell
kubectl exec -it <podName> -c <containerName> -- /bin/sh
```

# 31. Ã‡oklu container pod uygulama

> _k8sfundamentals/podmulticontainer.yaml dosyasÄ± Ã¶rneÄŸine bakabilirsiniz._

# 32. Init container

### Init Container ile bir Pod iÃ§erisinde birden fazla Container Ã‡alÄ±ÅŸtÄ±rma

Goâ€™daki `init()` komutu gibi ilk Ã§alÄ±ÅŸan containerâ€™dÄ±r. Ã–rneÄŸin, uygulama containerâ€™Ä±n baÅŸlayabilmesi iÃ§in bazÄ± config dosyalarÄ±nÄ± fetch etmesi gerekir. Bu iÅŸlemi init container iÃ§erisinde yapabiliriz.

1. Uygulama containerâ€™Ä± baÅŸlatÄ±lmadan Ã¶nce **Init Container** ilk olarak Ã§alÄ±ÅŸÄ±r.
2. Init Container yapmasÄ± gerekenleri yapar ve kapanÄ±r.
3. Uygulama containerâ€™Ä±, Init Container kapandÄ±ktan sonra Ã§alÄ±ÅŸmaya baÅŸlar. **Init Container kapanmadan uygulama containerâ€™Ä± baÅŸlamaz.**
**Ornek**: Uygulama containerin ihtiyacÄ± olan bazÄ± config dosyalarÄ±nÄ±n son gÃ¼ncel halinin uygulama baÅŸlamadan sisteme Ã§ekilmesi gerekiyor. Bu durumda ilk Ã¶nce init container ile Ã§ekme iÅŸini halledip daha sonra ana uygulama baÅŸlar.

![image](https://user-images.githubusercontent.com/103413194/184529084-bcc4ac7f-a223-4510-ae88-489e41a13af8.png)

> _k8sfundamentals/podinitcontainer.yaml dosyasÄ± Ã¶rneÄŸine bakabilirsiniz._

# 33. Label ve selector

## ğŸ· Label, Selector, Annotation

## Label Nedir?

* Label -> Etiket
* Selector -> Etiket SeÃ§me

Ã–R: `example.com/tier:front-end` â€“>`example.com/` = Prefix (optional) `tier` = **key**, `front-end` = **value**

* `kubernetes.io/`ve `k8s.io/` Kubernetes core bileÅŸenler iÃ§in ayrÄ±lmÄ±ÅŸtÄ±r, kullanÄ±lamazdÄ±r.
* Tire, alt Ã§izgi, noktalar iÃ§erebilir.
* TÃ¼rkÃ§e karakter kullanÄ±lamaz.
* **Service, deployment, pods gibi objectler arasÄ± baÄŸ kurmak iÃ§in kullanÄ±lÄ±r.**

# 34. Label ve selector uygulama

* Label tanÄ±mÄ± **metadata** tarafÄ±nda yapÄ±lÄ±r. AynÄ± objectâ€™e birden fazla label ekleyebiliriz. bir poddaki anahtara iki value atayamayÄ±z.
* Label, gruplandÄ±rma ve tanÄ±mlama imkanÄ± verir. CLI tarafÄ±nda listelemekte kolaylaÅŸÄ±r.

### Selector - Labelâ€™lara gÃ¶re Object Listelemek

* Ä°Ã§erisinde Ã¶rneÄŸin â€œappâ€ keyâ€™ine sahip objectleri listelemek iÃ§in:

```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod8
  labels:
    app: berk # app key burada. berk ise value'su.
    tier: backend # tier baÅŸka bir key, backend value'su.
...

![image](https://user-images.githubusercontent.com/103413194/184532721-92694678-8432-4a93-9c87-2947a57f9b36.png)

labels larÄ± kullanarak arama yapabiliyoruz.
---
obje oluÅŸtururken yamlÄ±n iÃ§inde obje yamlarÄ±nÄ±n arasÄ±nda --- koyarsak yeni bir obje oluÅŸturabiliriz.
```

![image](https://user-images.githubusercontent.com/103413194/184532768-38c3193e-f8bb-4596-a6db-22dad5388287.png)



![image](https://user-images.githubusercontent.com/103413194/184532830-49eb35f0-1a38-41b1-83df-daa6592d2fe4.png)


```shell
kubectl get pods -l <keyword> --show-labels

## Equality based Syntax'i ile listeleme

kubectl get pods -l "app" --show-labels

kubectl get pods -l "app=firstapp" --show-labels

kubectl get pods -l "app=firstapp, tier=front-end" --show-labels

# app key'i firstapp olan, tier'Ä± front-end olmayanlar:
kubectl get pods -l "app=firstapp, tier!=front-end" --show-labels

# app anahtarÄ± olan ve tier'Ä± front-end olan objectler:
kubectl get pods -l "app, tier=front-end" --show-labels

## Set based ile Listeleme

# App'i firstapp olan objectler:
kubectl get pods -l "app in (firstapp)" --show-labels

# app'i sorgula ve iÃ§erisinde "firstapp" olmayanlarÄ± getir:
kubectl get pods -l "app, app notin (firstapp)" --show-labels

kubectl get pods -l "app in (firstapp, secondapp)" --show-labels

# app anahtarÄ±na sahip olmayanlarÄ± listele
kubectl get pods -l "!app" --show-labels

# app olarak firstapp atanmÄ±ÅŸ, tier keyine frontend deÄŸeri atanmamÄ±ÅŸlarÄ± getir:
kubectl get pods -l "app in (firstapp), tier notin (frontend)" --show-labels
```

* Ä°lk syntaxâ€™ta (equality based) bir sonuÃ§ bulunamazken, 2. syntax (set based selector) sonuÃ§ gelir:

```yaml
kubectl get pods -l "app=firstapp, app=secondapp" --show-labels # SonuÃ§ yok!
kubectl get pods -l "app in (firstapp, secondapp)" --show-labels # SonuÃ§ var :)
```

### Komutla label ekleme

```shell
kubectl label pods <podName> <label>

kubectl label pods pod1 app=front-end
```

### Komutla label silme

Sonuna - (tire) koymak gerekiyor. Sil anlamÄ±na gelir.

```
kubectl label pods pod1 app-
```

### Komutla label gÃ¼ncelleme

```shell
kubectl label --overwrite pods <podName> <label>

kubectl label --overwrite pods pod9 team=team3
```

### Komutla toplu label ekleme

TÃ¼m objectlere bu label eklenir.

```
kubectl label pods --all foo=bar
```

## Objectler ArasÄ±nda Label Ä°liÅŸkisi

* NÅAâ€™da kube-sched kendi algoritmasÄ±na gÃ¶re bir node seÃ§imi yapar. EÄŸer bunu manuel hale getirmek istersek, aÅŸaÄŸÄ±daki Ã¶rnekte olduÄŸu gibi `hddtype: ssd` labelâ€™Ä±na sahip nodeâ€™u seÃ§mesini saÄŸlayabiliriz. BÃ¶ylece, pod ile node arasÄ±nda labelâ€™lar aracÄ±lÄ±ÄŸÄ±yla bir iliÅŸki kurmuÅŸ oluruz.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod11
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
  nodeSelector:
    hddtype: ssd
```

> _minikube clusterâ€™Ä± iÃ§erisindeki tek nodeâ€™a `hddtype: ssd` labelâ€™Ä± ekleyebiliriz. Bunu ekledikten sonra yukarÄ±daki pod â€œPendingâ€ durumundan, â€œRunningâ€ durumuna geÃ§ecektir. (AradÄ±ÄŸÄ± nodeâ€™u buldu Ã§Ã¼nkÃ¼_ :smile: _)_

```shell
kubectl label nodes minikube hddtype=ssd
```

# 35. Annotation

* AynÄ± label gibi davranÄ±r ve **metadata** altÄ±na yazÄ±lÄ±r.
* Labelâ€™lar 2 object arasÄ±nda iliÅŸki kurmak iÃ§in kullanÄ±ldÄ±ÄŸÄ±ndan hassas bilgi sÄ±nÄ±fÄ±na girer. (Label'lar bir durumu tetikleyebilir). Bu sebeple, label olarak kullanamayacaÄŸÄ±mÄ±z ama Ã¶nemli bilgileri **Annotation** sayesinde kayÄ±t altÄ±na alabiliriz.
* **example.com/notification-email:admin@k8s.com**
  * example.com â€“> Prefix (optional)
  * notification-email â€“> Key
  * admin@k8s.com â€“> Value

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: annotationpod
  annotations:
    owner: "Ozgur OZTURK"
    notification-email: "admin@k8sfundamentals.com"
    releasedate: "01.01.2021"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  containers:
  - name: annotationcontainer
    image: nginx
    ports:
    - containerPort: 80
```

### Komutla Annotation ekleme

```shell
kubectl annotate pods annotationpod foo=bar

kubectl annotate pods annotationpod foo- # Siler.
```
# Namespace

* 10 farklÄ± ekibin tek bir **file server** kullandÄ±ÄŸÄ± bir senaryo dÃ¼ÅŸÃ¼nelim:
  * Bir kiÅŸinin yarattÄ±ÄŸÄ± bir dosyayÄ± baÅŸkasÄ± overwrite edebilir ya da isim Ã§akÄ±ÅŸmasÄ±na sebep olabilir,
  * Sadece Team 1â€™in gÃ¶rmesi gereken dosyalarÄ± ayÄ±rmakta zorlanabilirim, sÃ¼rekli dosya ayarlarÄ± yapmam gerekir.
  * Bunun Ã§Ã¶zÃ¼mÃ¼ iÃ§in, her ekibe Ã¶zel bir klasÃ¶r yaratabilir ve permissionlarÄ±nÄ± ekip Ã¼yelerine gÃ¶re dÃ¼zenleyebiliriz.
* YukarÄ±daki Ã¶rnekteki **fileserver**â€™Ä± **k8s clusterÄ±**, **namespaceâ€™leri** ise burada her ekibe aÃ§Ä±lmÄ±ÅŸ **klasÃ¶rler** olarak dÃ¼ÅŸÃ¼nebiliriz.
* **Namespaceâ€™lerde birer k8s objectâ€™idir. TanÄ±mlarken (Ã¶zellikle YAML) dosyasÄ±nda buna gÃ¶re tanÄ±mlama yapÄ±lmalÄ±dÄ±r.**
* Namespaceâ€™lerin birbirinden baÄŸÄ±msÄ±z ve benzersiz olmasÄ± gerekir. **Namespaceâ€™ler birbiri iÃ§erisine yerleÅŸtirilemez.**
* Her k8s oluÅŸturulduÄŸunda ise 4 default namespace oluÅŸturulur. (_default, kube-node-lease, kube-public, kube-system_)

### Namespace Listeleme

* VarsayÄ±lan olarak tÃ¼m iÅŸlemler ve objectler **default namespace** altÄ±nda iÅŸlenir. `kubectl get pods`yazdÄ±ÄŸÄ±mÄ±zda herhangi bir namespace belirtmediÄŸimiz iÃ§in, `default namespace` altÄ±ndaki podlarÄ± getirir.

```shell
kubectl get pods --namespace <namespaceName>
kubectl get pods -n <namespaceName>

# TÃ¼m namespace'lerdeki podlarÄ± listelemek iÃ§in:
kubectl get pods --all-namespaces
```

### Namespace OluÅŸturma

```shell
kubectl create namespace <namespaceName>

kubectl get namespaces
```

#### YAML dosyasÄ± kullanarak Namespace oluÅŸturma

```yaml
apiVersion: v1
kind: Namespace # development isminde bir namespace oluÅŸturulur.
metadata:
  name: development # namespace'e isim veriyoruz.
---
apiVersion: v1
kind: Pod
metadata:
  namespace: development # oluÅŸturduÄŸumuz namespace altÄ±nda podu tanÄ±mlÄ±yoruz.
  name: namespacepod
spec:
  containers:
  - name: namespacecontainer
    image: nginx:latest
    ports:
    - containerPort: 80
```

Bir namespace iÃ§inde Ã§alÄ±ÅŸan Podâ€™u yaratÄ±rken ve bu poda baÄŸlanÄ±rken; kÄ±sacasÄ± bu podlar Ã¼zerinde herhangi bir iÅŸlem yaparken **namespace** belirtilmek zorundadÄ±r. Belirtilmezse, k8s ilgili podu **default namespace** altÄ±nda aramaya baÅŸlayacaktÄ±r.

### VarsayÄ±lan Namespaceâ€™i DeÄŸiÅŸtirmek

```
kubectl config set-context --current --namespace=<namespaceName>
```

### Namespaceâ€™i Silmek

:warning: **DÄ°KKAT!** **Namespace silerken confirmation istenmeyecektir. Namespace altÄ±ndaki tÃ¼m objectlerde silinecektir!**

```
kubectl delete namespaces <namespaceName>
```

# 37. Deployment

K8s kÃ¼ltÃ¼rÃ¼nde â€œSingleton (Tekil) Podâ€lar genellikle yaratÄ±lmaz. BunlarÄ± yÃ¶neten Ã¼st seviye objectâ€™ler yaratÄ±rÄ±z ve bu podlar bu objectler tarafÄ±ndan yÃ¶netilir. (Ã–R: Deployment)

**Peki, neden yaratmÄ±yoruz?**

Ã–rneÄŸin, bir frontend objectâ€™ini bir pod iÃ§erisindeki containerâ€™la deploy ettiÄŸimizi dÃ¼ÅŸÃ¼nelim. EÄŸer bu containerâ€™da hata meydana gelirse ve bizim RestartPolicyâ€™miz â€œAlways veya On-failureâ€ ise kube-sched containerÄ± yeniden baÅŸlatarak kurtarÄ±r ve Ã§alÄ±ÅŸmasÄ±na devam ettirir. Fakat, **sorun node Ã¼zerinde Ã§Ä±karsa, kube-sched, â€œBen bunu gidip baÅŸka bir worker-nodeâ€™da Ã§alÄ±ÅŸtÄ±rayÄ±mâ€ demez!**

Peki buna Ã§Ã¶zÃ¼m olarak 3 node tanÄ±mladÄ±k, Ã¶nlerine de bir load balancer koyduk. EÄŸer birine bir ÅŸey olursa diÄŸerleri online olmaya devam edeceÄŸi iÃ§in sorunu Ã§Ã¶zmÃ¼ÅŸ olduk. **AMA..** UygulamayÄ± geliÅŸtirdiÄŸimizi dÃ¼ÅŸÃ¼nelim. Tek tek tÃ¼m nodelardaki container imageâ€™larÄ±nÄ± yenilemek zorunda kalacaÄŸÄ±z. Label eklemek istesek, hepsine eklememiz gerekir. **Yani, iÅŸler karmaÅŸÄ±klaÅŸtÄ±.**

**Ã‡Ã–ZÃœM: â€œDeploymentâ€ Object**

* Deployment, bir veya birden fazla podâ€™u iÃ§in bizim belirlediÄŸimiz **desired state**â€™i sÃ¼rekli **current state**â€˜e getirmeye Ã§alÄ±ÅŸan bir object tipidir. Deploymentâ€™lar iÃ§erisindeki **deployment-controller** ile current stateâ€™i desired stateâ€™e getirmek iÃ§in gerekli aksiyonlarÄ± alÄ±r.
* Deployment objectâ€™i ile Ã¶rneÄŸin yukarÄ±daki image update etme iÅŸlemini tÃ¼m nodelarda kolaylÄ±kla yapabiliriz.
* Deploymentâ€™a iÅŸlemler sÄ±rasÄ±nda nasÄ±l davranmasÄ± gerektiÄŸini de (**Rollout**) parametre ile belirtebiliriz.
* **Deploymentâ€™ta yapÄ±lan yeni iÅŸlemlerde hata alÄ±rsak, bunu eski haline tek bir komutla dÃ¶ndÃ¼rebiliriz.**
* :warning: :warning: :warning: Ã–rneÄŸin, deployment oluÅŸtururken **replica** tanÄ±mÄ± yaparsak, k8s clusterâ€™Ä± her zaman o kadar replikaâ€™yÄ± canlÄ± tutmaya Ã§alÄ±ÅŸacaktÄ±r. Siz manuel olarak deploymentâ€™Ä±n oluÅŸturduÄŸu podâ€™lardan birini silseniz de, arka tarafta yeni bir pod ayaÄŸa kaldÄ±rÄ±lacaktÄ±r. Ä°ÅŸte bu sebeple biz **Singleton Pod** yaratmÄ±yoruz. Yani, manuel ya da yaml ile direkt pod yaratmÄ±yoruz ki bu optimizasyonu k8sâ€™e bÄ±rakÄ±yoruz.
* Tek bir pod yaratacak bile olsanÄ±z, bunu deployment ile yaratmalÄ±sÄ±nÄ±z! (k8s resmi Ã¶nerisi)

# 38. Deployment uygulama
### Komut ile Deployment OluÅŸturma

```shell
kubectl create deployment <deploymentName> --image=<imageName> --replicas=<replicasNumber>

kubectl create deployment <deploymentName> --image=nginx:latest --replicas=2

kubectl get deployment
# TÃ¼m deployment ready kolonuna dikkat!
```

### Deploymentâ€™taki imageâ€™Ä± Update etme

```shell
kubectl set image deployment/<deploymentName> <containerName>=<yeniImage>

kubectl set image deployment/firstdeployment nginx=httpd
```

* Default strateji olarak, Ã¶nce bir podâ€™u yeniler, sonra diÄŸerini, sonra diÄŸerini. Bunu deÄŸiÅŸtirebiliriz.

### Deployment Replicasâ€™Ä±nÄ± DeÄŸiÅŸtirme

```shell
kubectl scale deployment <deploymentName> --replicas=<yeniReplicaSayÄ±sÄ±>
```

### Deployment Silme

```shell
kubectl delete deployments <deploymentName>
```

### **YAML ile Deployment OluÅŸturma**

1. Herhangi bir pod oluÅŸturacak yaml dosyasÄ±ndaki **`metadata`** altÄ±nda kalan komutlarÄ± kopyala:

```yaml
# podexample.yaml

apiVersion: v1
kind: Pod
metadata:
  name: examplepod
  labels:
    app: frontend
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
```

1. Deployment oluÅŸturacak yaml dosyasÄ±nda **`template`** kÄ±smÄ±nÄ±n altÄ±na yapÄ±ÅŸtÄ±r. _(Indentâ€™lere dikkat!)_
2. **pod template iÃ§erisinden `name` alanÄ±nÄ± sil.**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: firstdeployment
  labels:
    team: development
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend # template iÃ§erisindeki pod'la eÅŸleÅŸmesi iÃ§in kullanÄ±lacak label.
  template:	    # OluÅŸturulacak podlarÄ±n Ã¶zelliklerini belirttiÄŸimiz alan.
    metadata:
      labels:
        app: frontend # deployment ile eÅŸleÅŸen pod'un label'i.
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80 # dÄ±ÅŸarÄ± aÃ§Ä±lacak port.
```

* Her deploymentâ€™ta **en az bir tane** `selector` tanÄ±mÄ± olmalÄ±dÄ±r.
* **Birden fazla deployment yaratacaksanÄ±z, farklÄ± labelâ€™lar kullanmak zorundasÄ±nÄ±z.** Yoksa deploymentlar hangi podlarÄ±n kendine ait olduÄŸunu karÄ±ÅŸtÄ±rabilir. **AyrÄ±ca, aynÄ± labellarÄ± kullanan singleton bir podâ€™da yaratmak sakÄ±ncalÄ±dÄ±r!**

# 39. ReplicaSet

K8sâ€™de x sayÄ±da pod oluÅŸturan ve bunlarÄ± yÃ¶neten object tÃ¼rÃ¼ aslÄ±nda **deployment deÄŸildir.** **ReplicaSet**, tÃ¼m bu iÅŸleri Ã¼stlenir. Biz deploymentâ€™a istediÄŸimiz derived stateâ€™i sÃ¶ylediÄŸimizde, deployments objectâ€™i bir ReplicaSet objectâ€™i oluÅŸturur ve tÃ¼m bu gÃ¶revleri ReplicaSet gerÃ§ekleÅŸtirir.

K8s ilk Ã§Ä±ktÄ±ÄŸÄ±nda **Replication-controller** adÄ±nda bir objectâ€™imiz vardÄ±. Halen var ama kullanÄ±lmÄ±yor.

```shell
kubectl get replicaset # Aktif ReplicaSet'leri listeler.
```

Bir deployment tanÄ±mlÄ±yken, Ã¼zerinde bir deÄŸiÅŸiklik yaptÄ±ÄŸÄ±mÄ±zda; deployment **yeni bir ReplicaSet** oluÅŸturur ve bu ReplicaSet yeni podlarÄ± oluÅŸturmaya baÅŸlar. Bir yandan da eski podlar silinir. **Fakat replicaset silinmez.**

### Deployment Ã¼zerinde yapÄ±lan deÄŸiÅŸiklikleri geri alma

```shell
kubectl rollout undo deployment <deploymentName>
```

Bu durumda ise eski deployment yeniden oluÅŸturulur ve eski ReplicaSet Ã¶nceki podlarÄ± oluÅŸturmaya baÅŸlar. Ä°ÅŸte bu sebeple, tÃ¼m bu iÅŸlemleri **manuel yÃ¶netmemek adÄ±na** bizler direkt ReplicaSet oluÅŸturmaz, iÅŸlemlerimize deployment oluÅŸturarak devam ederiz.

â€”> **Deployment > ReplicaSet > Pods**

* ReplicaSet, YAML olarak oluÅŸturulmak istendiÄŸinde **tamamen deployment ile aynÄ± ÅŸekilde oluÅŸturulur. Sadece kind kÄ±smÄ±na ReplicaSet yazÄ±lÄ±r.**
Replicaset yaml da image da yaptÄ±ÄŸÄ±mÄ±z bir deÄŸiÅŸikliÄŸi var olan podlara uygulamaz. Ancak yeni pod oluÅŸturacaksa yeni image dan oluÅŸturur. Bunun iÃ§in deployment objesini kullanÄ±yoruz.

# 40. Rollout ve Rollback

Rollout ve Rollback kavramlarÄ±, **deplomentâ€™Ä±n** gÃ¼ncellemesi esnasÄ±nda devreye girer, anlam kazanÄ±r.

**YAML** ile deployment tanÄ±mlamasÄ± yaparken **`strategy`** olarak 2 tip seÃ§ilir:

### Rollout Strategy - **`Recreate`**

```shell
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rcdeployment
  labels:
    team: development
spec:
  replicas: 3
  selector:
    matchLabels:
      app: recreate 
  strategy:
    type: Recreate # recreate === Rollout strategy
... 
```

* â€œ_Ben bu deploymentâ€™ta bir deÄŸiÅŸiklik yaparsam, Ã¶ncelikle tÃ¼m podlarÄ± sil, sonrasÄ±nda yenilerini oluÅŸtur._â€ Bu yÃ¶ntem daha Ã§ok **hardcore migration** yapÄ±ldÄ±ÄŸÄ±nda kullanÄ±lÄ±r.

UygulamamÄ±zÄ±n yeni versionuyla eski versionunun birlikte Ã§alÄ±ÅŸmasÄ± **sakÄ±ncalÄ±** ise bu yÃ¶ntem seÃ§ilir. ****&#x20;

**Ã–rneÄŸin,** bir RabbitMQ consumer'Ä±mÄ±z olduÄŸunu varsayalÄ±m. BÃ¶yle bir uygulamada eski version ve yeni version'un birlikte Ã§alÄ±ÅŸmasÄ± genellikle tercih edilen bir durum deÄŸildir. Bu sebeple, strategy olarak `recreate` tercih edilmelidir.

### Rollback Strategy - **`RollingUpdate`**

```shell
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rolldeployment
  labels:
    team: development
spec:
  replicas: 10
  selector:
    matchLabels:
      app: rolling
  strategy:
    type: RollingUpdate # Rollback Strategy
    rollingUpdate:
      maxUnavailable: 2 # GÃ¼ncelleme esnasÄ±nda aynÄ± anda kaÃ§ pod silineceÄŸi
      maxSurge: 2 # GÃ¼ncelleme esnasÄ±nda toplam aktif max pod sayÄ±sÄ±
  template:
  ...
```

* EÄŸer YAML dosyasÄ±nda strategy belirtmezseniz, **default olarak RollingUpdate seÃ§ilir.** **maxUnavailable ve maxSurge** deÄŸerleri ise default **%25â€™dir.**
* RollingUpdate, Createâ€™in tam tersidir. â€œBen bir deÄŸiÅŸiklik yaptÄ±ÄŸÄ±m zaman, hepsini silip; yenilerini **oluÅŸturma**.â€ Bu stratejiâ€™de Ã¶nemli 2 parametre vardÄ±r:
  * **`maxUnavailable`** â€“> En fazla burada yazÄ±lan sayÄ± kadar podâ€™u sil. Bir gÃ¼ncellemeye baÅŸlandÄ±ÄŸÄ± anda en fazla x kadar pod silinecek sayÄ±sÄ±. (%20 de yazabiliriz.)
  * **`maxSurge`** â€“> GÃ¼ncelleme geÃ§iÅŸ sÄ±rasÄ±nda sistemde toplamda kaÃ§ **max aktif podâ€™un olmasÄ± gerektiÄŸi sayÄ±dÄ±r.**

**Ã–rnek**

Bir deployment ayaÄŸa kaldÄ±rdÄ±ÄŸÄ±mÄ±zÄ± dÃ¼ÅŸÃ¼nelim. Image = nginx olsun. AÅŸaÄŸÄ±daki komut ile varolan deployment Ã¼zerinde gÃ¼ncelleme yapalÄ±m. nginx image'Ä± yerine httpd-alphine image'Ä±nÄ±n olmasÄ±nÄ± isteyelim:

```shell
kubectl set image deployment rolldeployment nginx=httpd-alphine --record=true
```

* `--record=true` parametresi bizim iÃ§in tÃ¼m gÃ¼ncelleme aÅŸamalarÄ±nÄ± kaydeder. Ã–zellikle, bir Ã¶nceki duruma geri dÃ¶nmek istediÄŸimizde iÅŸe yarar.

### YapÄ±lan deÄŸiÅŸikliklerin listelenmesi

```shell
# rolldeployment = deploymentName
# tÃ¼m deÄŸiÅŸiklik listesi getirilir.
kubectl rollout history deployment rolldeployment 

# nelerin deÄŸiÅŸtiÄŸini spesifik olarak gÃ¶rmek iÃ§in:
kubectl rollout history deployment rolldeployment --revision=2
```

### YapÄ±lan deÄŸiÅŸikliklerin geri alÄ±nmasÄ±

```shell
# rolldeployment = deploymentName
# Bir Ã¶nceki duruma geri dÃ¶nmek iÃ§in:
kubectl rollout undo deployment rolldeployment

# Spesifik bir revision'a geri dÃ¶nmek iÃ§in:
kubectl rollout undo deployment rolldeployment --to-revision=1
```

### CanlÄ± olarak deployment gÃ¼ncellemeyi izlemek

```shell
# rolldeployment = deploymentName
kubectl rollout status deployment rolldeployment -w 
```

### Deployment gÃ¼ncellemesi esnasÄ±nda pauseâ€™lamak

GÃ¼ncelleme esnasÄ±nda bir problem Ã§Ä±ktÄ± ve geri de dÃ¶nmek istemiyorsak, ayrÄ±ca sorunun nereden kaynaklandÄ±ÄŸÄ±nÄ± da tespit etmek istiyorsak kullanÄ±lÄ±r.

```shell
# rolldeployment = deploymentName
kubectl rollout pause deployment rolldeployment
```

### Pauseâ€™lanan deployment gÃ¼ncellemesini devam ettirmek

```shell
kubectl rollout resume deployment rolldeployment
```

# 41. Kubernetes aÄŸ altyapÄ±sÄ±
K8s Temel AÄŸ AltyapÄ±sÄ±
AÅŸaÄŸÄ±daki Ã¼Ã§ kural dahilinde K8s network yapÄ±sÄ± ele alÄ±nmÄ±ÅŸtÄ±r (olmazsa olmazdÄ±r):

K8s kurulumda podâ€™lara ip daÄŸÄ±tÄ±lmasÄ± iÃ§in bir IP adres aralÄ±ÄŸÄ± (--pod-network-cidr) belirlenir.
K8sâ€™te her pod bu cidr bloÄŸundan atanacak unique bir IP adresine sahip olur.
AynÄ± cluster iÃ§erisindeki Podâ€™lar default olarak birbirleriyle herhangi bir kÄ±sÄ±tlama olmadan ve NAT (Network Address Translation) olmadan haberleÅŸebilir.

![image](https://user-images.githubusercontent.com/103413194/184542131-0cb8a680-1b6f-4524-a257-e51c82735701.png)

K8s iÃ§erisindeki containerlar 3 tÃ¼r haberleÅŸmeye maruz bÄ±rakÄ±lÄ±r:
Bir container k8s dÄ±ÅŸÄ±ndaki bir IP ile haberleÅŸir,
Bir container kendi node iÃ§erisindeki baÅŸka bir containerâ€™la haberleÅŸir,
Bir container farklÄ± bir node iÃ§erisindeki baÅŸka bir containerâ€™la haberleÅŸir.
Ä°lk 2 senaryoda sorun yok ama 3. senaryoâ€™da NAT konusunda problem yaÅŸanÄ±r. Bu sebeple k8s containerlarÄ±n birbiri ile haberleÅŸme konusunda Container Network Interface (CNI) projesini devreye almÄ±ÅŸtÄ±r.
CNI, yanlÄ±zca containerlarÄ±n aÄŸ baÄŸlantÄ±sÄ±yla ve containerlar silindiÄŸinde containerlara ayrÄ±lan kaynaklarÄ±n drop edilmesiyle ilgilenir.
K8s ise aÄŸ haberleÅŸme konusunda CNI standartlarÄ±nÄ± kabul etti ve herkesin kendi seÃ§eceÄŸi CNI pluginlerinden birini seÃ§mesine izin verdi. AÅŸaÄŸÄ±daki adreslerden en uygun olan CNI pluginini seÃ§ebilirsiniz:
{% embed url="https://github.com/containernetworking/cni" %}

{% embed url="https://kubernetes.io/docs/concepts/cluster-administration/networking" %}

Containerâ€™larÄ±n â€œDÄ±ÅŸ DÃ¼nyaâ€ ile haberleÅŸmesi konusunu ele aldÄ±k. Peki, â€œDÄ±ÅŸ DÃ¼nyaâ€, Containerâ€™lar ile nasÄ±l haberleÅŸecek?

Cevap: Service objectâ€™i.

# 42. Service
43. K8s network tarafÄ±nÄ± ele alan objecttir.
Service Objecti Ã‡Ä±kÄ±ÅŸ Senaryosu
1 Frontend (React), 1 Backend (Go) oluÅŸan bir sistemimiz olduÄŸunu dÃ¼ÅŸÃ¼nelim:

Her iki uygulama iÃ§in birer deployment yazdÄ±k ve 3â€™er pod tanÄ±mlanmasÄ±nÄ± saÄŸladÄ±k.
3 Frontend poduna dÄ±ÅŸ dÃ¼nyadan nasÄ±l eriÅŸim saÄŸlayacaÄŸÄ±m?
Frontendâ€™den gelen istek backendâ€™de iÅŸlenmeli. Burada Ã§ok bir problem yok. Ã‡Ã¼nkÃ¼, her podâ€™un bir IP adresi var ve K8s iÃ§erisindeki her pod birbiriyle bu IP adresleri sayesinde haberleÅŸebilir.
Bu haberleÅŸmeyi saÄŸlayabilmek iÃ§in; Frontend podlarÄ±nÄ±n Backend podlarÄ±nÄ±n IP adreslerini bilmeleri gerekir. Bunu Ã§Ã¶zmek iÃ§in, frontend deploymentâ€™Ä±na tÃ¼m backend podlarÄ±nÄ±n IP adreslerini yazdÄ±m. Fakat, podâ€™lar gÃ¼ncellenebilir, deÄŸiÅŸebilir ve bu gÃ¼ncellemeler esnasÄ±nda yeni bir IP adresi alabilir. Yeni oluÅŸan IP adreslerini tekrar Frontend deploymentâ€™Ä±nda tanÄ±mlamam gerekir.
Ä°ÅŸte tÃ¼m bu durumlarÄ± Ã§Ã¶zmek iÃ§in Service objecti yaratÄ±rÄ±z. K8s, Podâ€™lara kendi IP adreslerini ve bir dizi Pod iÃ§in tek bir DNS adÄ± verir ve bunlar arasÄ±nda yÃ¼k dengeler.

Service Tipleri
ClusterIP (Containerâ€™lar ArasÄ±)
â€“> Bir ClusterIP serviceâ€™i yaratÄ±p, bunu labelâ€™lar sayesinde podlarla iliÅŸkilendirebiliriz. Bu objecti yarattÄ±ÄŸÄ±mÄ±z zaman, Cluster iÃ§erisindeki tÃ¼m podlarÄ±n Ã§Ã¶zebileceÄŸi unique bir DNS adrese sahip olur. AyrÄ±ca, her k8s kurulumunda sanal bir IP rangeâ€™e sahip olur. (Ã–r: 10.0.100.0/16)

â€“> ClusterIP service objecti yaratÄ±ldÄ±ÄŸÄ± zaman, bu objectâ€™e bu IP aralÄ±ÄŸÄ±ndan bir IP atanÄ±r ve ismiyle bu IP adresi Clusterâ€™Ä±n DNS mekanizmasÄ±na kaydedilir. Bu IP adresi Virtual (Sanal) bir IP adresidir.

â€“> Kube-proxy tarafÄ±ndan tÃ¼m nodeâ€™lardaki IP Tableâ€™a bu IP adresi eklenir. Bu IP adresine gelen her trafik Round Troppin algoritmasÄ±yla Podâ€™lara yÃ¶nlendirilir. Bu durum bizim 2 sÄ±kÄ±ntÄ±mÄ±zÄ± Ã§Ã¶zer:

Ben Frontend nodeâ€™larÄ±na bu IP adresini tanÄ±mlayÄ±p (ismine de backenddersem), Backendâ€™e gitmen gerektiÄŸi zaman bu IP adresini kullan diyebilirim. (Selector = app:backend) Tek tek her seferinde Frontend podlarÄ±na Backend podlarÄ±nÄ±n IP adreslerini eklemek zorunda kalmam!
ğŸ‰ ğŸ‰ Ã–zetle: ClusterIP Serviceâ€™i Containerâ€™larÄ± arasÄ±ndaki haberleÅŸmeyi, Service Discovery ve Load Balancing gÃ¶revi Ã¼stlenerek Ã§Ã¶zer!\

NodePort Service (DÄ±ÅŸ DÃ¼nya -> Container)
â€“> Bu service tipi, Cluster dÄ±ÅŸÄ±ndan gelen baÄŸlantÄ±larÄ± Ã§Ã¶zmek iÃ§in kullanÄ±lÄ±r.

â€“> NodePort keyâ€™i kullanÄ±lÄ±r.\

![image](https://user-images.githubusercontent.com/103413194/184542656-4e79bd34-d1b9-45f4-8f32-bcfcb5abdeb1.png)

LoadBalancer Service (Cloud Serviceâ€™leri Ä°Ã§in)
â€“> Bu service tipi, sadece Agent K8s, Google K8s Engine gibi yerlerde kullanÄ±lÄ±r.\

![image](https://user-images.githubusercontent.com/103413194/184542671-6ec1f693-7ba1-433d-b471-31f9eacd1f9c.png)

ExternalName Service (Åuan iÃ§in gereksiz.)


# 43. Service uygulama

### Cluster IP Ã–rneÄŸi

apiVersion: v1
kind: Service
metadata:
  name: backend # Service ismi
spec:
  type: ClusterIP # Service tipi
  selector:
    app: backend # Hangi podla ile eÅŸleÅŸeceÄŸi (pod'daki labella aynÄ±)
  ports:
    - protocol: TCP
      port: 5000
      targetPort: 5000
Herhangi bir object, cluster iÃ§erisinde oluÅŸan clusterIP:5000 e istek attÄ±ÄŸÄ±nda karÅŸÄ±lÄ±k bulacaktÄ±r.

Ã–nemli Not: Serviceâ€™lerin ismi oluÅŸturulduÄŸunda ÅŸu formatta olur: serviceName.namespaceName.svc.cluster.domain

EÄŸer aynÄ± namespaceâ€™de baÅŸka bir object bu servise gitmek istese core DNS Ã§Ã¶zÃ¼mlemesi sayesinde direkt backend yazabilir. BaÅŸka bir namespaceden herhangi bir object ise ancak yukarÄ±daki uzun ismi kullanarak bu servise ulaÅŸmalÄ±dÄ±r.

NodePort Ã–rneÄŸi
UnutulmamalÄ±dÄ±r ki, tÃ¼m oluÅŸan NodePort servislerinin de ClusterIPâ€™si mevcuttur. Yani, iÃ§eriden bu ClusterIP kullanÄ±larak bu servisle konuÅŸulabilir.
minikube service â€“url <serviceName> ile minikube kullanÄ±rken tunnel aÃ§abiliriz. Ã‡Ã¼nkÃ¼, biz normalde bu worker nodeâ€™un iÃ§erisine dÄ±ÅŸardan eriÅŸemiyoruz. Bu tamamen minikube ile alakalÄ±dÄ±r.
apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  type: NodePort
  selector:
    app: frontend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
Load Balancer Ã–rneÄŸi
Google Cloud Service, Azure Ã¼zerinde oluÅŸturulan clusterâ€™larda Ã§alÄ±ÅŸacak bir servistir.

apiVersion: v1
kind: Service
metadata:
  name: frontendlb
spec:
  type: LoadBalancer
  selector:
    app: frontend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
Imperative Olarak Service OluÅŸturma
kubectl delete service <serviceName>

# ClusterIP Service yaratmak
kubectl expose deployment backend --type=ClusterIP --name=backend

kubectl expose deployment backend --type=NodePort --name=backend
Endpoint Nedir?
NasÄ±l deploymentâ€™lar aslÄ±nda ReplicaSet oluÅŸturduysa, Service objectleride arka planda birer Endpoint oluÅŸturur. Serviceâ€™lerimize gelen isteklerin nereye yÃ¶nleneceÄŸi Endpointâ€™ler ile belirlenir.

kubectl get endpoints
Bir pod silindiÄŸinde yeni oluÅŸacak pod iÃ§in, yeni bir endpoint oluÅŸturulur.

# Liveness, Readiness, Resource Limits, Env. Variables

	
# âš¡ Volume, Secret, ConfigMap
	
# 44. Liveness probes

EÄŸer container Ã§alÄ±ÅŸmassa kubelet contaÄ±nerÄ± kapatÄ±p tekrar kuruyor. Fakat ayakta olmasÄ±na raÄŸmen containerin gÃ¶revini yerine getirmiyorsa kubelet bunu gÃ¶remiyor. DoÄŸal olarak containeri kapatÄ±p yeniden aÃ§mayÄ± denemiyor. Bu durumda liveless probes ile Ã§Ã¶zebiliyoruz.
	
	![image](https://user-images.githubusercontent.com/103413194/185057781-f4ca40d9-de1c-462d-a10f-df0d72ad7481.png)

apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/liveness
    args:
    - /server
    livenessProbe:
      httpGet:
        path: /healthz
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: Awesome
      initialDelaySeconds: 3
      periodSeconds: 3
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: k8s.gcr.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5
---
apiVersion: v1
kind: Pod
metadata:
  name: goproxy
  labels:
    app: goproxy
spec:
  containers:
  - name: goproxy
    image: k8s.gcr.io/goproxy:0.1
    ports:
    - containerPort: 8080
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
	
# 45. Readiness probes

	deployment Ä±le yeni imagelar ayaga kaldÄ±rdÄ±m. Bunlar oluÅŸunca service-load balancer a baglanacak ve internet sÄ±temden gelen requestlere cevap verecek. eger contaÄ±ner calÄ±sÄ±yorsa fakat Ä±nternet sÄ±temÄ± sunmaya hazÄ±r degÄ±lse ne olacak? veya load balancer servisinden trafik almaya hazÄ±r olduÄŸunu nasÄ±l anlayacaÄŸÄ±m. 

	![image](https://user-images.githubusercontent.com/103413194/185067925-5fb05741-3fbe-4546-bd3b-db3a2e2b2a46.png)

	![image](https://user-images.githubusercontent.com/103413194/185068817-857270fc-a4cb-4805-a353-393b53592f1b.png)

	apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  labels:
    team: development
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: frontend
        image: ozgurozturknet/k8s:blue
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /healthcheck
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 80
          initialDelaySeconds: 20
          periodSeconds: 3
---
apiVersion: v1
kind: Service
metadata:
  name: frontend
spec:
  selector:
    app: frontend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
	
# 46. Resource limits
	
	containerimÄ±zÄ± bir namespacenin iÃ§ine kurduk. EÄŸer containerin memory ve cpu suna bir sÄ±nÄ±rlama getirmezsek namespace simizin tÃ¼m kaynaÄŸÄ±nÄ± kullanacaktÄ±r. 
	bundan dolayÄ± request(istenilen) ve limits(kullanabileceÄŸi son seviye) kriterleri belirleriz. EÄŸer memory belÄ±rlenen limite gelirse bunu gecemiyeceÄŸinden yeniden pod oluÅŸturulacaktÄ±r.
	
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: requestlimit
  name: requestlimit
spec:
  containers:
  - name: requestlimit
    image: ozgurozturknet/stress
    resources:
      requests:
        memory: "64M"
        cpu: "250m"
      limits:
        memory: "256M"
        cpu: "0.5"

![image](https://user-images.githubusercontent.com/103413194/185073755-ffcc6220-1b37-43bf-8556-9b95114b5cc5.png)

![image](https://user-images.githubusercontent.com/103413194/185073807-a9079974-02fe-493d-a465-4f840932fb15.png)

#	47. Environment variable

Environment variables uygulamamÄ±zÄ±n kodu dÄ±ÅŸÄ±nda bulunur, uygulamamÄ±zÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ± yerde kullanÄ±labilir. UygulamalarÄ±mÄ±zÄ±n yapÄ±landÄ±rmasÄ±nÄ± kodundan ayÄ±rmak iÃ§in kullanÄ±labilir, bu da uygulamalarÄ±mÄ±zÄ±n farklÄ± ortamlara kolayca daÄŸÄ±tÄ±lmasÄ±nÄ± saÄŸlar. Node.js uygulamalarÄ±nda, ortam deÄŸiÅŸkenleri process.env genel deÄŸiÅŸkeni aracÄ±lÄ±ÄŸÄ±yla kullanÄ±labilir. .Env dosyasÄ± hiÃ§bir zaman kaynak kodu deposunda olmamalÄ±dÄ±r.
Environment variables Ä±mage hard code olarak gÃ¶mmeyÄ±z. container oluÅŸtururken env bÃ¶lÃ¼mÃ¼ altÄ±nda bunlarÄ± tanÄ±mlarÄ±z.
	
apiVersion: v1
kind: Pod
metadata:
  name: envpod
  labels:
    app: frontend
spec:
  containers:
  - name: envpod
    image: ozgurozturknet/env:latest
    ports:
    - containerPort: 80
    env:
      - name: USER
        value: "Ozgur"
      - name: database
        value: "testdb.example.com"
	
![image](https://user-images.githubusercontent.com/103413194/185083812-facb0cc7-243f-48fe-a061-7dd0d9debde4.png)


# 48. Volume

* Containerâ€™Ä±n dosyalarÄ± container varolduÄŸu sÃ¼rece bir dosya iÃ§erisinde tutulur. EÄŸer container silinirse, bu dosyalarda birlikte silinir. Her yeni bir container yaratÄ±ldÄ±ÄŸÄ±nda, containerâ€™a ait dosyalar yeniden yaratÄ±lÄ±r. (Stateless kavramÄ±)
* BazÄ± containerlarda bu dosyalarÄ±n silinmemesi (tutulmasÄ±) gerekebilir. Ä°ÅŸte bu durumda **Ephemeral (GeÃ§ici) Volume kavramÄ±** devreye giriyor. (Stateful kavramÄ±) Ã–rneÄŸin; DB containerâ€™Ä±nda data kayÄ±plarÄ±na uÄŸramamak iÃ§in **volume** kurgusu yapÄ±lmalÄ±dÄ±r.
* Ephemeral Volumeâ€™ler aynÄ± pod iÃ§erisindeki tÃ¼m containerlara aynÄ± anda baÄŸlanabilir.
* DiÄŸer bir amaÃ§ ise aynÄ± pod iÃ§erisindeki birden fazla containerâ€™Ä±n ortak kullanabileceÄŸi bir alan yaratmaktÄ±r.

> Ephemeral Volumeâ€™lerde Pod silinirse tÃ¼m veriler kaybolur. Fakat, container silinip tekrar yaratÄ±lÄ±rsa Podâ€™a bir ÅŸey olmadÄ±ÄŸÄ± sÃ¼rece veriler saklanÄ±r.

2 tip Ephemeral Volume vardÄ±r:

### emptyDir Volume

![](.gitbook/assets/image-20220101232209717.png)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: emptydir
spec:
  containers:
  - name: frontend
    image: ozgurozturknet/k8s:blue
    ports:
    - containerPort: 80
    livenessProbe:
      httpGet:
        path: /healthcheck
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5
    volumeMounts:
    - name: cache-vol # volume ile baÄŸlantÄ± saÄŸlamak iÃ§in.
      mountPath: /cache # volume iÃ§erisindeki hangi dosya?
  - name: sidecar
    image: busybox
    command: ["/bin/sh"]
    args: ["-c", "sleep 3600"]
    volumeMounts:
    - name: cache-vol
      mountPath: /tmp/log # volume iÃ§erisindeki hangi dosya?
  volumes:
  - name: cache-vol # Ã–nce volume'Ã¼ oluÅŸturur sonra container'lara tanÄ±mlarÄ±z.
    emptyDir: {}
```

### hostPath Volume

* Ã‡ok nadir durumlarda kullanÄ±lÄ±r, kullanÄ±lÄ±rken dikkat edilmelidir.
* emptyDirâ€™da volume klasÃ¶rÃ¼ pod iÃ§erisinde yaratÄ±lÄ±rken, hostPathâ€™te volume klasÃ¶rÃ¼ worker node iÃ§erisinde yaratÄ±lÄ±r.
* 3 farklÄ± tipâ€™te kullanÄ±lÄ±r:
  * **Directory** â€“> Worker node Ã¼zerinde zaten var olan klasÃ¶rler iÃ§in kullanÄ±lÄ±r.
  * **DirectoryOrCreate** â€“> Zaten var olan klasÃ¶rler ya da bu klasÃ¶r yoksa yaratÄ±lmasÄ± iÃ§in kullanÄ±lÄ±r.
  * **FileOrCreate** â€“> KlasÃ¶r deÄŸil! Tek bir dosya iÃ§in kullanÄ±lÄ±r. Yoksa yaratÄ±lÄ±r.

![](.gitbook/assets/image-20220101232320593.png)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hostpath
spec:
  containers:
  - name: hostpathcontainer
    image: ozgurozturknet/k8s:blue
    ports:
    - containerPort: 80
    livenessProbe:
      httpGet:
        path: /healthcheck
        port: 80
      initialDelaySeconds: 5
      periodSeconds: 5
    volumeMounts:
    - name: directory-vol
      mountPath: /dir1
    - name: dircreate-vol
      mountPath: /cache
    - name: file-vol
      mountPath: /cache/config.json       
  volumes:
  - name: directory-vol
    hostPath:
      path: /tmp
      type: Directory
  - name: dircreate-vol
    hostPath:
      path: /cache
      type: DirectoryOrCreate
  - name: file-vol
    hostPath:
      path: /cache/config.json
      type: FileOrCreate
```

## 49. Secret

* Hassas bilgileri (DB User, Pass vb.) Environment Variables iÃ§erisinde saklayabilsekte, bu yÃ¶ntem ideal olmayabilir.
* Secret objectâ€™i sayesinde bu hassas bilgileri uygulamanÄ±n object tanÄ±mlarÄ±nÄ± yaptÄ±ÄŸÄ±mÄ±z YAML dosyalarÄ±ndan ayÄ±rÄ±yoruz ve ayrÄ± olarak yÃ¶netebiliyoruz.
* Token, username, password vb. tÃ¼m bilgileri secret iÃ§erisinde saklamak her zaman daha gÃ¼venli ve esnektir.
	
![image](https://user-images.githubusercontent.com/103413194/184618725-f653c641-e11b-45c8-b744-28d1d5e02f00.png)

![image](https://user-images.githubusercontent.com/103413194/184619377-38df4602-a948-42a8-8ef7-825dfe7faa74.png)
	
	![image](https://user-images.githubusercontent.com/103413194/184619600-669203cd-ad0e-4a93-9fe9-bfba71961697.png)
	
![image](https://user-images.githubusercontent.com/103413194/184632990-f37a5574-b3a8-48b8-9eae-8bfc96d5847e.png)

### Birinci dosyanÄ±n yerine 2. dosyayÄ± yazmÄ±ÅŸ oluyoruz. AmaÃ§ hassas bilgileri korumak.
### Declarative Secret OluÅŸturma

* AtayacaÄŸÄ±mÄ±z secret ile oluÅŸturacaÄŸÄ±mÄ±z podâ€™lar **aynÄ± namespace Ã¼zerinde olmalÄ±dÄ±r.**
* 8 farklÄ± tipte secret oluÅŸturabiliriz. **`Opaque`** generic bir typeâ€™dÄ±r ve hemen hemen her hassas datamÄ±zÄ± bu tipi kullanarak saklayabiliriz.
	
	![image](https://user-images.githubusercontent.com/103413194/184633752-04949cc4-0157-429f-90dd-68245a71eeee.png)

dosyayÄ± nereden aldÄ±ÄŸÄ±nÄ± gÃ¶rÃ¼yoruz.

Ã–rnek bir secret.yaml dosyasÄ±:

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
stringData:		# Hassas data'larÄ± stringData altÄ±na yazÄ±yoruz.
  db_server: db.example.com
  db_username: admin
  db_password: P@ssw0rd!
```
stringData kÄ±smÄ±nÄ± Data olarak da oluÅŸturabiliriz. Fakat tanÄ±mlama farklÄ± olur.

Secret iÃ§erisindeki datalarÄ± gÃ¶rmek iÃ§in:

```shell
kubectl describe secrets <secretName>
```

### Imperative Secret OluÅŸturma

```shell
kubectl create secret generic <secretName> --from-literal=db_server=db.example.com --from-literal=db_username=admin
```

:warning: Buradaki `generic` demek yamlâ€™da yazdÄ±ÄŸÄ±mÄ±z `Opaque` â€˜a denk gelmektedir.

â€“> EÄŸer CLI Ã¼zerinde hassas verileri girmek istemiyorsak, her bir veriyi ayrÄ± bir `.txt` dosyasÄ±na girip; `â€“from-file=db_server=server.txt` komutunu Ã§alÄ±ÅŸtÄ±rabiliriz. `.txt` yerine `.json`da kullanabiliriz. O zaman `â€“from-file=config.json` yazmalÄ±yÄ±z.
	
```shell
kubectl create secret generic <secretName> --from-file=db_server=server.txt --from-file=db_username=username.txt --from-file=db_password=password.txt
```
```shell
kubectl create secret generic <secretName> --from-file=config.json
```
burada config.json key in adÄ± ve dosyanÄ±n iÃ§indeki de value sÄ± oluyor.
Json Ã¶rneÄŸi:

```yaml
{
   "apiKey": "6bba108d4b2212f2c30c71dfa279e1f77cc5c3b2",
}
```

### Podâ€™dan Secretâ€™Ä± Okuma

â€“> OluÅŸturduÄŸumuz Secretâ€™larÄ± Podâ€™a aktarmak iÃ§in 2 yÃ¶ntem vardÄ±r:

â€‹ **Volume olarak aktarma** ve **Env variable olarak aktarma**

Her iki yÃ¶ntemi de aÅŸaÄŸÄ±daki YAML dosyasÄ±nda bulabilirsiniz:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secretpodvolume
spec:
  containers:
  - name: secretcontainer
    image: ozgurozturknet/k8s:blue
    volumeMounts: # 2) OluÅŸturulan secret'lÄ± volume'Ã¼ pod'a dahil ediyoruz.
    - name: secret-vol
      mountPath: /secret # 3) Uygulama iÃ§erisinde /secret klasÃ¶rÃ¼ volume'e dahil olmuÅŸtur. 
      # ArtÄ±k uygulama iÃ§erisinden bu dosyaya ulaÅŸÄ±p, deÄŸerleri okuyabiliriz.
  volumes:				# 1) Ã–nce volume'Ã¼ oluÅŸturuyoruz ve secret'Ä± volume'e dahil ediyoruz.
  - name: secret-vol
    secret:
      secretName: mysecret3 
      # Bu pod'un iÃ§erisine exec ile girdiÄŸimizde root altÄ±nda secret klasÃ¶rÃ¼ gÃ¶receÄŸiz. Buradaki dosyalarÄ±n ismi "KEY", iÃ§indeki deÄŸerler "VALUE"dur. 
---
apiVersion: v1
kind: Pod
metadata:
  name: secretpodenv
spec:
  containers:
  - name: secretcontainer
    image: ozgurozturknet/k8s:blue
    env:	# TÃ¼m secretlarÄ± pod iÃ§erisinde env. variable olarak tanÄ±mlayabiliriz.
    # Bu yÃ¶ntemde, tÃ¼m secretlarÄ± ve deÄŸerlerini tek tek tanÄ±mladÄ±k.
      - name: username
        valueFrom:
          secretKeyRef:
            name: mysecret3 # mysecret3 isimli secret'Ä±n "db_username" key'li deÄŸerini al.
            key: db_username
      - name: password
        valueFrom:
          secretKeyRef:
            name: mysecret3
            key: db_password
      - name: server
        valueFrom:
          secretKeyRef:
            name: mysecret3
            key: db_server
---
apiVersion: v1
kind: Pod
metadata:
  name: secretpodenvall
spec:
  containers:
  - name: secretcontainer
    image: ozgurozturknet/k8s:blue
    envFrom: # 2. yÃ¶ntemle aynÄ±, sadece tek fark tÃ¼m secretlarÄ± tek bir seferde tanÄ±mlÄ±yoruz.
    - secretRef:
        name: mysecret3
```

**Podâ€™lardaki TÃ¼m Env Variableâ€™larÄ± GÃ¶rmek**

```shell
kubectl exec <podName> -- printenv
```

# 50. ConfigMap

* ConfigMapâ€™ler Secret objectleriyle tamamen aynÄ± mantÄ±kta Ã§alÄ±ÅŸÄ±r. Tek farkÄ±; Secretâ€™lar etcd Ã¼zerinde base64 ile encoded edilerek encrypted bir ÅŸekilde saklanÄ±r. ConfigMapâ€™ler ise encrypted edilmez ve bu sebeple hassas datalar iÃ§ermemelidir.
* Pod iÃ§erisine **Volume** veya **Env. Variables** olarak tanÄ±mlayabiliriz.
* OluÅŸturulma yÃ¶ntemleri Secret ile aynÄ± olduÄŸundan yukarÄ±daki komutlar geÃ§erlidir.

	![image](https://user-images.githubusercontent.com/103413194/184618994-d77e558b-7a55-4c96-b6b5-9caece1dbd17.png)


	![image](https://user-images.githubusercontent.com/103413194/184619084-3bb761bc-e73a-4f90-82cf-23978c7c4cbb.png)

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: myconfigmap
data: # Key-Value ÅŸeklinde girilmelidir.
  db_server: "db.example.com"
  database: "mydatabase"
  site.settings: | # Birden fazla satÄ±r yazÄ±mÄ± iÃ§in "|" kullanÄ±lÄ±r.
    color=blue
    padding:25px
---
apiVersion: v1
kind: Pod
metadata:
  name: configmappod
spec:
  containers:
  - name: configmapcontainer
    image: ozgurozturknet/k8s:blue
    env:
      - name: DB_SERVER
        valueFrom:
          configMapKeyRef:
            name: myconfigmap
            key: db_server
      - name: DATABASE
        valueFrom:
          configMapKeyRef:
            name: myconfigmap
            key: database
    volumeMounts:
      - name: config-vol
        mountPath: "/config"
        readOnly: true
  volumes:
    - name: config-vol
      configMap:
        name: myconfigmap
```

### :small\_orange\_diamond::small\_blue\_diamond: `config.yaml` DosyasÄ±ndan ConfigMap OluÅŸturma

UygulamamÄ±z iÃ§erisinde kullanÄ±lmak Ã¼zere her bir environment iÃ§in (QA, SIT ve PROD) `config.qa.yaml` ya da `config.prod.json` dosyamÄ±zÄ±n olduÄŸunu dÃ¼ÅŸÃ¼nelim. CI/CD iÃ§erisinde bu environmentlara gÃ¶re Ã§alÄ±ÅŸacak doÄŸru config dosyamÄ±zdan ConfigMap nasÄ±l oluÅŸturabiliriz?

```json
// config.json
{
  "name": "Berko",
  "surName": "Safran",
  "email": "berk@safran.com",
  "apiKey": "6bba108d4b2212f2c30c71dfa279e1f77cc5c3b2",
  "text": ["test", "deneme", "bir", "ki", 3, true]
}
```

1.  Kubectl aracÄ±lÄ±ÄŸÄ±yla config.json dosyamÄ±zdan ConfigMap objectâ€™i oluÅŸturalÄ±m.

    **EÄŸer bir CI/CD Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±yor ve loglarÄ± takip edeceksek;**

```shell
# --dry-run normalde deprecated, fakat hala eski versionlarda kullanÄ±lÄ±yor.
kubectl create configmap xyzconfig --from-file ${configFile} -o yaml --dry-run | kubectl apply -f -

# yeni hali --dry-run="client"
kubectl create configmap berkconfig --from-file config.json -o yaml --dry-run="client" | kubectl apply -f -

# Sondaki "-" (dash) Ã§Ä±kan pipe'Ä±n ilk tarafÄ±ndan Ã§Ä±kan output'u alÄ±r.
```

* YukarÄ±daki komutu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±zda kubectl config.json dosyasÄ±nÄ± alÄ±r; bize `kubectl apply` komutunu kullanabileceÄŸimiz bir ConfigMap YAML dosyasÄ± iÃ§eriÄŸini oluÅŸturur ve bu iÃ§eriÄŸi **`â€“dry-run`** seÃ§eneÄŸinden dolayÄ± **output olarak ekrana basar.**
* Gelen outputâ€™u alÄ±p (bash â€œpipe | â€ ile) `kubectl apply` komutu ile clusterâ€™Ä±mÄ±za gÃ¶nderiyoruz ve ConfigMap tanÄ±tÄ±mÄ±nÄ± yapÄ±yoruz.

**EÄŸer loglarÄ± okumadan direkt config.json dosyasÄ±ndan bir ConfigMap yaratmak istiyorsak;**

```shell
# config.json yerine bir Ã§ok formatta dosya kullanabiliriz: Ã–R: yaml
kubectl create configmap <configName> --from-file config.json
```

2\. Åimdi Podâ€™u oluÅŸtururken ConfigMapâ€™imizdeki deÄŸerleri Pod iÃ§erisinde yer alan bir klasÃ¶rdeki bir dosyaya aktarmamÄ±z ve bunu â€œvolumeâ€ mantÄ±ÄŸÄ±nda bir dosya iÃ§erisinde saklamamÄ±z gerekiyor.

* â€œvolumesâ€ kÄ±smÄ±nda volumeâ€™Ã¼mÃ¼zÃ¼ ve configMapâ€™imizi tanÄ±mlayalÄ±m.
* Pod iÃ§erisindeki â€œvolumeMountsâ€ kÄ±smÄ±nda ise tanÄ±mladÄ±ÄŸÄ±mÄ±z bu volumeâ€™Ã¼ podâ€™umuza tanÄ±talÄ±m.
  * **`mountPath`** kÄ±smÄ±nda configMap iÃ§erisinde yer alan dosyanÄ±n, Pod iÃ§erisinde hangi klasÃ¶r altÄ±na kopyalanacaÄŸÄ± ve yeni isminin ne olacaÄŸÄ±nÄ± belirtebiliriz.
  * **`subPath`** ile configMap iÃ§erisindeki dosyanÄ±n (Ã–R: `config.json`) ismini vermemiz gerekiyor. BÃ¶ylelikle Pod yaratÄ±lÄ±rken â€œBu dosyanÄ±n ismi deÄŸiÅŸecek.â€ demiÅŸ oluyoruz.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmappod4
spec:
  containers:
  - name: configmapcontainer
    image: ozgurozturknet/k8s:blue
    volumeMounts:
      - name: config-vol
        mountPath: "/config/newconfig.json"
        subPath: "config.json"
        readOnly: true
  volumes:
    - name: config-vol
      configMap:
        name: test-config # Bu ConfigMap iÃ§erisinde config.json dosyasÄ± vardÄ±r.
```

#### **Volumesâ€™Ã¼n farklÄ± yazÄ±mÄ±**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmappod
spec:
  containers:
  - name: configmapcontainer
    image: ozgurozturknet/k8s:blue
    volumeMounts:
      - name: config-vol
        mountPath: "/config/newconfig.json"
        subPath: "config.json"
        readOnly: true
  volumes:
    - name: config-vol
      projected:
        sources:
        - configMap:
            name: test-config
            items:
              - key: config.json
                path: config.json
```

# 51. Node Affinity
	
![image](https://user-images.githubusercontent.com/103413194/184653159-07ee616e-bf58-4a44-9b72-ef6f7c81f2bb.png)

apiVersion: v1
kind: Pod
metadata:
  name: nodeaffinitypod1
spec:
  containers:
  - name: nodeaffinity1
    image: ozgurozturknet/k8s
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: app
            operator: In #In, NotIn, Exists, DoesNotExist
            values:
            - blue
---
apiVersion: v1
kind: Pod
metadata:
  name: nodeaffinitypod2
spec:
  containers:
  - name: nodeaffinity2
    image: ozgurozturknet/k8s
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: app
            operator: In
            values:
            - blue
      - weight: 2
        preference:
          matchExpressions:
          - key: app
            operator: In
            values:
            - red
---
apiVersion: v1
kind: Pod
metadata:
  name: nodeaffinitypod3
spec:
  containers:
  - name: nodeaffinity3
    image: ozgurozturknet/k8s
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: app
            operator: Exists #In, NotIn, Exists, DoesNotExist

affinity pod un altÄ±ndaki spec kÄ±smÄ±nda olusturulur. 
**requiredDuringSchedulingIgnoredDuringExecution**
	
podaffinity de amaÃ§ belirlenen (Ã¶rneÄŸin key: app ve values: blue) anahtarÄ± nodun labelÄ±nda arar. varsa o nodun altÄ±nda podu oluÅŸturur yoksa oluÅŸturmaz.
operator de ise node selector den ayrÄ±ÅŸmamÄ±zÄ± ve daha fazla iÅŸ yapabilmemizi saÄŸlÄ±yor. 
	operatorde #In, NotIn, Exists, DoesNotExist
Ã–rneÄŸin operatorde **In**  yazÄ±yorsa (key: app ve values: blue) labela sahÄ±p node da schedule edÄ±yor.
Ã–rneÄŸin operatorde **NotIn** yazÄ±yorsa (key: app ve values: blue) labela **sahip olmayan** node da schedule edÄ±yor. buna unaffinity denir.
Ã–rneÄŸin operatorde **Exists** yazÄ±yorsa (key: app ) labela da olmasi yeterli olacakti.
Ã–rneÄŸin operatorde **Exists** yazÄ±yorsa (key: app ) labela da olmamasi gerekir.
	
**preferredDuringSchedulingIgnoredDuringExecution:**
weight degerine gÃ¶re Ã¶ncelik oluÅŸuyor. Weight 1 ile 100 arasÄ±nda deÄŸer alÄ±yor. deÄŸeri yÃ¼ksek olan oluÅŸuyor. 
YukarÄ±daki yaml dosyasÄ±ndan yola Ã§Ä±karsak pod, weight 2 deki key ve value deÄŸerlerini saÄŸlayan node da oluÅŸturulacak eÄŸer bÃ¶yle bir node yoksa weight 1 deki key ve value deÄŸerlerine bakacak. EÄŸer onu da bulamazsa herhangi bir node da oluÅŸturacak.
**preferredDuringSchedulingIgnoredDuringExecution:** ifadesinin iÃ§indeki IgnoreDuringExecution aslÄ±nda pod oluÅŸturulduktan sonra nodeworker daki label deÄŸiÅŸse de pod un Ã§alÄ±ÅŸmaya devam etmesi anlamÄ±na geliyor.
Not: Yaml dosyasÄ±nda 1. pod sadece key ve value deÄŸerini saÄŸlÄ±yorsa oluÅŸacak. 2. pod her kriterlere gÃ¶re her halÃ¼karda oluÅŸacak ve 3. pod operator kÄ±smÄ± Exist olarak tanÄ±mlÄ± olduÄŸu iÃ§in nodeworker da label da key:app olarak tanÄ±mlÄ±ysa oluÅŸacak.
	
Not2: Bu durumda sadece 2. pod oluÅŸur.
	
![image](https://user-images.githubusercontent.com/103413194/185093686-6228bc7c-a606-45a0-a4d6-3c3360f298c9.png)

komutunu yazarsam 1. ve 3. pod da Ã§alÄ±ÅŸacaktÄ±r.
	
52. Pod Affinity
EÄŸer biz podu hangi kriterlere sahip node Ã¼zerinde oluÅŸturmamÄ±zÄ± belirleyeceksek nodeaffinity yi kullanÄ±rÄ±z. Fakat eÄŸer biz pod u oluÅŸtururken diÄŸer podlarÄ± da kriter olarak kabul ediyorsak o zaman podaffinity i kullanmalÄ±yÄ±z.
	
	![image](https://user-images.githubusercontent.com/103413194/185098987-e3adcd6a-5ac3-431a-a869-6e723370a8b5.png)

Bu kurgudan yola Ã§Ä±karsak, frondend ve database  farklÄ± az lerde oluÅŸmuÅŸ. bu Ã¼cretlendirilen bir durum. Bunun Ã§Ã¶zÃ¼mÃ¼ ben komut olarak git database hangi az de ise veya worknode da oluÅŸturulduysa frondend orada oluÅŸtur.
	
![image](https://user-images.githubusercontent.com/103413194/185099636-569f3766-a29f-48f2-b147-68e2dc852eec.png)

apiVersion: v1
kind: Pod
metadata:
  name: frontendpod
  labels:
    app: frontend
    deployment: test
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: cachepod
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - frontend
        topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: color
              operator: In
              values:
              - blue
          topologyKey: kubernetes.io/hostname
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: deployment
              operator: In
              values:
              - prod
          topologyKey: topology.kubernetes.io/zone
  containers:
  - name: cachecontainer
    image: redis:6-alpine
	

pod frontendpod da topologkey e bakÄ±ldÄ±gÄ±nda hostnamesi(nodeworker) aynÄ± olan yerde Ã§alÄ±ÅŸtÄ±rÄ±lacak.
podantÄ±affÄ±nÄ±ty de ise ÅŸunu diyoruz: key:deployment ve value:prod olan zone iÃ§erisinde Ã§alÄ±ÅŸtÄ±rma. baÅŸka bir yerde Ã§alÄ±ÅŸtÄ±r.
	

	
	


	
	
